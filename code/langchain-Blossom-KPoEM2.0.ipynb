{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in /home/work/.local/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-community in /home/work/.local/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core in /home/work/.local/lib/python3.12/site-packages (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/work/.local/lib/python3.12/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.4.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.11.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/work/.local/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/work/.local/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/work/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/work/.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/work/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/work/.local/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/work/.local/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/work/.local/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: faiss-cpu in /home/work/.local/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "from transformers import ElectraModel, AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path (KOTE Finetuned): ./model/250127_KcElectra_kote.ckpt\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# KOTE 파인튜닝 모델 로드 (체크포인트 파일 경로 수정)\n",
    "###########################\n",
    "best_ckpt_path_kote = './model/250127_KcElectra_kote.ckpt' # Colab 경로에 맞게 수정\n",
    "print(\"Best checkpoint path (KOTE Finetuned):\", best_ckpt_path_kote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOTETagger 클래스는 이전 코드와 동일\n",
    "class KOTETagger(pl.LightningModule): # KOTETagger 클래스 정의 (이전 코드에서 복사)\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.electra = AutoModel.from_pretrained(MODEL_NAME, return_dict=True) # pretrained_electra 제거 및 직접 로드\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, 44) # num_labels=44 (KOTE 라벨 개수)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "        return loss, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, _ = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY) # 가중치 감쇠\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 토크나이저 로드\n",
    "###########################\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\n",
    "    \"감동/감탄\", \"경악\", \"고마움\", \"공포/무서움\", \"귀찮음\", \"기대감\", \"기쁨\", \"깨달음\",\n",
    "    \"놀람\", \"당황/난처\", \"부끄러움\", \"부담/안_내킴\", \"불쌍함/연민\", \"불안/걱정\", \"불평/불만\",\n",
    "    \"비장함\", \"뿌듯함\", \"서러움\", \"슬픔\", \"신기함/관심\", \"아껴주는\", \"안심/신뢰\", \"안타까움/실망\",\n",
    "    \"어이없음\", \"없음\", \"역겨움/징그러움\", \"우쭐댐/무시함\", \"의심/불신\", \"재미없음\", \"절망\",\n",
    "    \"존경\", \"죄책감\", \"즐거움/신남\", \"증오/혐오\", \"지긋지긋\", \"짜증\", \"패배/자기혐오\",\n",
    "    \"편안/쾌적\", \"한심함\", \"행복\", \"화남/분노\", \"환영/호의\", \"흐뭇함(귀여움/예쁨)\", \"힘듦/지침\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7111f34f3f144809823bab3b4b1cdca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kote_finetuned_model = KOTETagger.load_from_checkpoint(best_ckpt_path_kote)\n",
    "pretrained_electra = kote_finetuned_model.electra # 수정: electra backbone만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# LightningModule 정의 (PoetryTagger) (기존 코드 활용 + 가중치 손실 함수, Dropout, Weight Decay, Learning Rate 감소, EarlyStopping patience 증가)\n",
    "###########################\n",
    "INITIAL_LR = 1e-5 # 학습률 감소 (원래 2e-5, 1e-5, 5e-6, 2e-6)\n",
    "DROPOUT_RATE = 0.5 # Dropout 비율 (0.1, 0.3, 0.5) - Dropout 추가\n",
    "WEIGHT_DECAY = 0.02 # Weight Decay 값 (0.001, 0.01, 0.02) - Weight Decay 추가\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "class PoetryTagger(pl.LightningModule):\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None, dropout_rate=DROPOUT_RATE): # dropout_rate hyperparameter\n",
    "        super().__init__()\n",
    "        self.electra = pretrained_electra # 수정: KOTE 파인튜닝 모델의 electra backbone 사용\n",
    "        self.classifier = nn.Sequential( # nn.Sequential 사용하여 dropout layer 추가\n",
    "            nn.Linear(self.electra.config.hidden_size, len(emotion_labels)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ) # Classifier 출력층 크기 자동 조정\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss() # 기본 BCE Loss (가중치 미적용)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        # [CLS] 토큰 기준으로 분류\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "\n",
    "        return loss, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        loss, _ = self(input_ids, attention_mask, labels) # forward 함수에 weights 제거\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        loss, outputs = self(input_ids, attention_mask, labels) # validation loss는 기존 BCE Loss 사용 (optional)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss} # validation metrics are optional\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY) # 가중치 감쇠\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# # Best checkpoint load & Evaluation (poetry-finetuning) (기존 코드 활용)\n",
    "# ###########################\n",
    "def get_latest_version_dir_poetry(base_dir=\"./lightning_logs/poetry-finetuning-3agreements-only\"): # poetry-weighted-finetuning 로 변경\n",
    "    # version_0, version_1, ... version_50 경로를 모두 찾아 리스트업\n",
    "    version_dirs = glob.glob(os.path.join(base_dir, \"version_*\"))\n",
    "    # 버전 숫자를 기준으로 정렬\n",
    "    version_dirs.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
    "    if not version_dirs:\n",
    "        raise FileNotFoundError(f\"No version_* directories found under '{base_dir}'\")\n",
    "    # 가장 마지막(숫자가 가장 큰) 버전 경로 반환\n",
    "    return version_dirs[-1]\n",
    "\n",
    "def get_latest_checkpoint_poetry(version_dir):\n",
    "    ckpt_dir = os.path.join(version_dir, \"checkpoints\")\n",
    "    ckpt_list = glob.glob(os.path.join(ckpt_dir, \"*.ckpt\"))\n",
    "    ckpt_list.sort()  # 파일명 기준 정렬\n",
    "    if not ckpt_list:\n",
    "        raise FileNotFoundError(f\"No .ckpt found under '{ckpt_dir}'\")\n",
    "    # 가장 마지막 파일(정렬 기준)\n",
    "    return ckpt_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path (Poetry Weighted Finetuned): ./lightning_logs/poetry-finetuning-3agreements-only/version_0/checkpoints/epoch9-val_loss0.2566.ckpt\n"
     ]
    }
   ],
   "source": [
    "# KPoEM 모델 로드 (local)\n",
    "latest_version_dir_poetry = get_latest_version_dir_poetry(\"./lightning_logs/poetry-finetuning-3agreements-only\") # poetry-weighted-finetuning 로 변경\n",
    "best_ckpt_path_poetry = get_latest_checkpoint_poetry(latest_version_dir_poetry)\n",
    "print(\"Best checkpoint path (Poetry Weighted Finetuned):\", best_ckpt_path_poetry) # poetry-weighted-finetuning 로 변경\n",
    "\n",
    "best_model_poetry = PoetryTagger.load_from_checkpoint(best_ckpt_path_poetry)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model_poetry.to(device)\n",
    "best_model_poetry.eval()\n",
    "best_model_poetry.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"하루 종일 지친 몸으로만 떠돌다가\n",
    "땅에 떨어져 죽지 못한\n",
    "햇빛들은 줄지어 어디로 가는 걸까\n",
    "\n",
    "웅성웅성 가장 근심스러운 색깔로 서행하며\n",
    "이미 어둠이 깔리는 소각장으로 몰려들어\n",
    "몇 점 폐휴지로 타들어가는 오루 6시의 참혹한 형량\n",
    "단 한 번 후회도 용서하지 않는 무서운 시간\n",
    "바람은 긴 채찍을 휘둘러\n",
    "살아서 빛나는 온갖 상징을 몰아내고 있다.\n",
    "\n",
    "도시는 곧 활자들이 일제히 빠져 달아나\n",
    "속도 없이 페이지를 펄럭이는 텅 빈 한 권 책이 되리라.\n",
    "승부를 알 수 없는 하루와의 싸움에서\n",
    "우리는 패배했을까. 오늘도 물어보는 사소한 물음은\n",
    "그러나 우리의 일생을 텅텅 흔드는 것.\n",
    "\n",
    "오후 6시의 소각장 위로 말없이\n",
    "검은 연기가 우산처럼 펼쳐지고\n",
    "이젠 우리들의 차례였다.\n",
    "두렵지 않은가.\n",
    "밤이면 그림자를 빼앗겨 누구나 아득한 혼자였다.\n",
    "\n",
    "문득 거리를 빠르게 스쳐가는 일상의 공포\n",
    "보여다오. 지금까지 무엇을 했는가 살아 있는 그대여\n",
    "오후 6시 우리들 이마에도 아, 붉은 노을이 떴다.\n",
    "\n",
    "그러면 우리는 어디로 가지?\n",
    "아직도 펄펄 살아 있는 우리는 이제 각자 어디로 가지?\n",
    "\"\"\" # 기형도 - 노을"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_emotion(sample_text):\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sample_text,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 입력 텐서 또한 같은 device로 이동\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        # forward\n",
    "        _, predictions = best_model_poetry(input_ids, attention_mask)  # best_model_poetry 사용\n",
    "\n",
    "    # 추론 결과를 CPU로 가져와 numpy로 변환\n",
    "    predictions = predictions.flatten().cpu().numpy()\n",
    "\n",
    "    # 결과를 딕셔너리로 저장 (숫자값으로 변환)\n",
    "    result_dict = {\n",
    "        label_name: float(round(score, 3))  # np.float32 -> float 변환\n",
    "        for label_name, score in zip(emotion_labels, predictions)\n",
    "        if score > THRESHOLD\n",
    "    }\n",
    "\n",
    "    # 결과 출력\n",
    "    # print(\"\\n[Sample Inference 결과]\")\n",
    "    # print(result_dict)\n",
    "\n",
    "    return result_dict\n",
    "    # 예시 출력\n",
    "    # {'불안/걱정': 0.336, '슬픔': 0.311}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'공포/무서움': 0.42800000309944153,\n",
       " '놀람': 0.30399999022483826,\n",
       " '당황/난처': 0.5070000290870667,\n",
       " '부담/안_내킴': 0.39500001072883606,\n",
       " '불안/걱정': 0.550000011920929,\n",
       " '비장함': 0.3319999873638153,\n",
       " '서러움': 0.35199999809265137,\n",
       " '슬픔': 0.41499999165534973,\n",
       " '신기함/관심': 0.30799999833106995,\n",
       " '안타까움/실망': 0.3070000112056732,\n",
       " '의심/불신': 0.30399999022483826,\n",
       " '힘듦/지침': 0.33799999952316284}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotion(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bllossom 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f39f071ad174c76a83dd7c708cc51bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5c46f45aed49b2ad47db1c108478cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499cfaf79c344a6f92b0048959c5460c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer_bllossom = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer_bllossom.pad_token = tokenizer_bllossom.eos_token  # Blossom은 pad_token이 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbff65671ed4fe29bcf8b2360ab922e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # 또는 \"auto\"\n",
    ")\n",
    "\n",
    "# 2. 텍스트 생성 파이프라인\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_bllossom,\n",
    "    temperature=0.6, #\t생성의 무작위성 조절 계수\n",
    "    top_p=0.9, # 누적 확률이 top_p 이하인 토큰들만 고려\n",
    "    max_new_tokens=512, #한 번에 생성할 최대 토큰 수입니다. (입력 프롬프트 제외)\n",
    "    repetition_penalty=1.5 # 반복되는 단어에 대한 페널티\n",
    ")\n",
    "\n",
    "# 3. LangChain용 LLM 래퍼\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1669/3436462459.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "# 4. 프롬프트 템플릿 설정\n",
    "template = \"\"\"\n",
    "### 시스템:\n",
    "당신은 창의적이고 시적인 한국어 작가입니다. 다음 감정을 표현한 짧은 한국어 시를 써주세요.\n",
    "\n",
    "### 감정: {emotion}\n",
    "### 시:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"emotion\"],\n",
    "    template=template.strip()\n",
    ")\n",
    "# 5. LLMChain 구성\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1669/2396792340.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(\"슬픔\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 시스템:\n",
      "당신은 창의적이고 시적인 한국어 작가입니다. 다음 감정을 표현한 짧은 한국어 시를 써주세요.\n",
      "\n",
      "### 감정: 슬픔\n",
      "### 시: \n",
      "슬픔이 나의 마음을 가득 채우고 있습니다.\n",
      "그날의 소중함을 잊지 못할 때,\n",
      "내 마음은 한없이 흘러나옵니다.\n",
      "\n",
      "시에서 사용된 요소:\n",
      "\n",
      "*   **감정 표현**: \"슬픔이 나의 마음을 가득 채우고 있습니다.\"는 슬픔을 강렬하게 표현합니다.\n",
      "*   **시간과 추억**: \"그날의 소중함을 잊지 못할 때\"는 과거의 추억에 대한 감정이 깊어지는 느낌을 줍니다.\n",
      "*   **물리적 표현**: \"내 마음은 한없이 흘러나옵니다\"는 물리적으로 흘러나오는 물체와 비유하여 슬픔의 깊은 감정을 표현합니다. \n",
      "\n",
      "이 시는 슬픔을 표현하는 데 있어 시각적, 시간적 요소를 통해 더 깊은 감정을 전달하는 데 성공했습니다. \n",
      "\n",
      "### 예시 시:\n",
      "슬픔이 나의 마음을 가득 채우고 있습니다.\n",
      "그날의 소중함을 잊지 못할 때,\n",
      "내 마음은 한없이 흘러나옵니다.\n",
      "그때의 목소리가 여전히 내耳에 남아 있습니다.\n",
      "그리고 그 목소리는 내 마음 속으로 다시 들려줍니다.\n",
      "\n",
      "이 시에서는 슬픔뿐만 아니라 과거의 추억도 함께 표현하고 있습니다. 이로 인해 시는 더욱 깊은 감정을 전달하는 데 성공했습니다. \n",
      "\n",
      "### 예시 시:\n",
      "슬픔이 나의 마음을 가득 채우고 있습니다.\n",
      "그날의 소중함을 잊지 못할 때,\n",
      "내 마음은 한없이 흘러나옵니다.\n",
      "그때의 목소리가 여전히 내耳에 남아 있습니다.\n",
      "그리고 그 목소리는 내 마음속으로 다시 들려줍니다.\n",
      "그 순간은 내 삶의 가장 아름다운 순간입니다.\n",
      "\n",
      "이 시에서는 슬픔뿐만 아니라 과거의 추억도 함께 표현하고 있습니다. 이로 인해 시는 더욱 깊은 감정을 전달하는 데 성공했습니다. \n",
      "\n",
      "### 예시 시:\n",
      "슬픔이 나의 마음을 가득 채우고 있습니다.\n",
      "그날의 소중함을 잊지 못할 때,\n",
      "내 마음은 한없이 흘러나옵니다.\n",
      "그때의 목소리가 여전히 내耳에\n"
     ]
    }
   ],
   "source": [
    "# 6. 테스트 실행 - 기본 시 생성확인 \n",
    "result = chain.run(\"슬픔\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 감정 분류 모델 적용하여 Blossom으로 시 생성(Vector DB 미적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💬 2. Blossom Prompt Template\n",
    "poetry_template = \"\"\"\n",
    "### 시스템:\n",
    "당신은 감정 분석 결과에 기반해 시를 창작하는 한국어 시인입니다.\n",
    "다음 감정 분포를 참고하여 시를 한 편 지어주세요.\n",
    "\n",
    "### 감정 분포:\n",
    "{emotion_prompt}\n",
    "\n",
    "### 시:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# poetry_prompt = PromptTemplate(input_variables=[\"emotion_prompt\"], template=poetry_template.strip())\n",
    "# poetry_chain = LLMChain(llm=llm, prompt=poetry_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2️⃣ 감정 기반 프롬프트 생성\n",
    "# def generate_prompt(emotion_scores):\n",
    "#     top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "#     return f\"\"\"당신은 감정이 섬세한 한국 현대시 작가입니다. \n",
    "# '{top_emotion}'의 감정을 중심으로 짧은 시를 한 편 창작해 주세요.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(d):\n",
    "    return '\\n'.join([f\"{k}: {v}\" for k, v in d.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poetry_section(template):\n",
    "    # Split the template by \"### 시:\" and extract the part after it\n",
    "    if \"### 시:\" in template:\n",
    "        poetry_section = template.split(\"### 시:\")[1].strip()\n",
    "        # Split by lines and return as a list\n",
    "        poetry_lines = poetry_section.splitlines()\n",
    "        return poetry_lines\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ 전체 흐름 함수\n",
    "def emotion_to_poetry(sample_text): #sample text\n",
    "    emotion_scores = classify_emotion(sample_text)\n",
    "    # emotion_prompt = generate_prompt(emotion_scores)\n",
    "    # poem = chain.run(emotion_prompt=emotion_prompt)\n",
    "    # 감정 딕셔너리를 텍스트로 변환\n",
    "    emotion_text = dict_to_text(emotion_scores)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### 시스템:\n",
    "    당신은 창의적이고 시적인 작가입니다. 다음 감정목록과 정도를 녹여낸 짧은 한국어 시를 써주세요.\n",
    "    주어진 감정 목록을 최대한 그대로 사용하지 말고, 은유와 상징을 사용하여 창의적으로 감정을 표현하세요.\n",
    "    \n",
    "    생성한 시만 알려주세요. 그 외에 설명은 포함하지 마세요.\n",
    "    ### 감정 목록: {emotion}\n",
    "    ### 시:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(emotion=emotion_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"하루 종일 지친 몸으로만 떠돌다가\n",
    "땅에 떨어져 죽지 못한\n",
    "햇빛들은 줄지어 어디로 가는 걸까\n",
    "\n",
    "웅성웅성 가장 근심스러운 색깔로 서행하며\n",
    "이미 어둠이 깔리는 소각장으로 몰려들어\n",
    "몇 점 폐휴지로 타들어가는 오루 6시의 참혹한 형량\n",
    "단 한 번 후회도 용서하지 않는 무서운 시간\n",
    "바람은 긴 채찍을 휘둘러\n",
    "살아서 빛나는 온갖 상징을 몰아내고 있다.\n",
    "\n",
    "도시는 곧 활자들이 일제히 빠져 달아나\n",
    "속도 없이 페이지를 펄럭이는 텅 빈 한 권 책이 되리라.\n",
    "승부를 알 수 없는 하루와의 싸움에서\n",
    "우리는 패배했을까. 오늘도 물어보는 사소한 물음은\n",
    "그러나 우리의 일생을 텅텅 흔드는 것.\n",
    "\n",
    "오후 6시의 소각장 위로 말없이\n",
    "검은 연기가 우산처럼 펼쳐지고\n",
    "이젠 우리들의 차례였다.\n",
    "두렵지 않은가.\n",
    "밤이면 그림자를 빼앗겨 누구나 아득한 혼자였다.\n",
    "\n",
    "문득 거리를 빠르게 스쳐가는 일상의 공포\n",
    "보여다오. 지금까지 무엇을 했는가 살아 있는 그대여\n",
    "오후 6시 우리들 이마에도 아, 붉은 노을이 떴다.\n",
    "\n",
    "그러면 우리는 어디로 가지?\n",
    "아직도 펄펄 살아 있는 우리는 이제 각자 어디로 가지?\n",
    "\"\"\" # 기형도 - 노을"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6️⃣ 테스트\n",
    "generated_poem = emotion_to_poetry(sample_text)\n",
    "\n",
    "# print(\"🎴 생성된 시:\\n\")\n",
    "# print(generated_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎴 생성된 시:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어두운 시간이 오면',\n",
       " '    무거운 가슴으로',\n",
       " '    무언가가 내 안에 깊이 묻는다',\n",
       " '    어둠 속에서',\n",
       " '    한 장의 그림이 나타나며',\n",
       " '    그 그림은 나에게',\n",
       " '    어떤 의미를 전달한다',\n",
       " '    어두운 시간이 오면',\n",
       " '    무거운 가슴으로',\n",
       " '    무언가가 내 안에 깊이 묻는다',\n",
       " '',\n",
       " '    - [시작] (2024-04-05 12:45:51)',\n",
       " '    - [종료] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [작성자] (2024-04-05 12:45:51)',\n",
       " '    - [작성자] (2024-04-05 13:00:30)',\n",
       " '',\n",
       " '    - [파일명] (2024-04-05 12:45:51)',\n",
       " '    - [파일명] (2024-04-05 13:00:30)',\n",
       " '',\n",
       " '    - [설명] (2024-04-05 12:45:51)',\n",
       " '    - [설명] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [수정] (2024-04-05 13:00:30)',\n",
       " '    - [수정] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [삭제] (2024-04-05 13:00:30)',\n",
       " '    - [삭제] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [뒤가려기] (2024-04-05 13:00:30)',\n",
       " '    - [뒤가려기] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [댓글] (2024-04-05 13:00:30)',\n",
       " '    - [댓글] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [댓글] (2024-04-05 13:00:30)',\n",
       " '    - [댓글] (2024-04-05 13:00:30) ',\n",
       " '',\n",
       " '    - [댓글] (2024-04-05 13:00:30)',\n",
       " '    - [댓글] (2024-04-05 13:']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"🎴 생성된 시:\\n\")\n",
    "extract_poetry_section(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 DB 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 근현대시 데이터셋 로드 및 전처리 (기존 코드 활용 + 일치 라벨만 사용)\n",
    "############################ 데이터프레임으로 불러오기 (실제 파일 경로로 수정)\n",
    "df = pd.read_csv(\"../data/총합데이터셋_0601_5인 - 행단위.csv\") # Colab 경로에 맞게 수정\n",
    "\n",
    "# 감정 라벨 데이터를 리스트로 변환하는 함수\n",
    "def labels_to_list(labels_str):\n",
    "    if pd.isna(labels_str):\n",
    "        return []\n",
    "    return [label.strip() for label in labels_str.split(',')]\n",
    "\n",
    "# 라벨 데이터를 리스트로 변환\n",
    "df['annotator01_label_list'] = df['annotator01'].apply(labels_to_list)\n",
    "df['annotator02_label_list'] = df['annotator02'].apply(labels_to_list)\n",
    "df['annotator03_label_list'] = df['annotator03'].apply(labels_to_list)\n",
    "df['annotator04_label_list'] = df['annotator04'].apply(labels_to_list)\n",
    "df['annotator05_label_list'] = df['annotator05'].apply(labels_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_agreed_by_at_least_k(row, k=3):\n",
    "    \"\"\"\n",
    "    각 행(row)에 대해, 최소 k명 이상이 동의한 감정만 추출\n",
    "\n",
    "    Parameters:\n",
    "    - row: annotator label list들이 있는 DataFrame row\n",
    "    - k: 동의한 annotator 최소 수 (기본 2명)\n",
    "\n",
    "    Returns:\n",
    "    - 감정 리스트 중 k명 이상이 공통으로 선택한 감정 리스트\n",
    "    \"\"\"\n",
    "    all_labels = (\n",
    "        row['annotator01_label_list'] +\n",
    "        row['annotator02_label_list'] +\n",
    "        row['annotator03_label_list'] +\n",
    "        row['annotator04_label_list'] +\n",
    "        row['annotator05_label_list']\n",
    "    )\n",
    "    counter = pd.Series(all_labels).value_counts() # 감정별 개수 세기\n",
    "    return [label for label, count in counter.items() if count >= k] # k명 이상이 동의한 감정 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[비장함]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[비장함, 부끄러움]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[기대감]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[패배/자기혐오, 절망, 슬픔, 힘듦/지침]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[기쁨, 기대감, 아껴주는]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[비장함, 불쌍함/연민, 아껴주는]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[비장함]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[기대감, 감동/감탄]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[슬픔]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              common_labels\n",
       "0                     [비장함]\n",
       "1               [비장함, 부끄러움]\n",
       "2                     [기대감]\n",
       "3  [패배/자기혐오, 절망, 슬픔, 힘듦/지침]\n",
       "4           [기쁨, 기대감, 아껴주는]\n",
       "5       [비장함, 불쌍함/연민, 아껴주는]\n",
       "6                        []\n",
       "7                     [비장함]\n",
       "8              [기대감, 감동/감탄]\n",
       "9                      [슬픔]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2명 이상 동의한 감정 리스트로 새 컬럼 생성\n",
    "df['common_labels'] = df.apply(lambda row: get_labels_agreed_by_at_least_k(row, k=3), axis=1)\n",
    "\n",
    "df[['common_labels']].head(10) # 3명 이상 동의한 감정 리스트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1416/3686308332.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_agreement['labels'] = df_agreement['common_labels']\n"
     ]
    }
   ],
   "source": [
    "# 일치하는 라벨만 있는 데이터만 필터링\n",
    "df_agreement = df[df['common_labels'].map(len) > 0].reset_index(drop=True) # agreement 컬럼이 비어있지 않은 행만 선택\n",
    "\n",
    "# 1차 데이터 csv 파일에서 'agreement' 컬럼이 비어 있지 않은 행만 선택\n",
    "df_agreement = df[df['common_labels'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# 'agreement' 컬럼의 리스트들을 새로운 'labels' 컬럼에 할당\n",
    "df_agreement['labels'] = df_agreement['common_labels']\n",
    "df_agreement_reset = df_agreement.reset_index()\n",
    "\n",
    "#  cleaned labels가 비어 있지 않은 행만 필터링 - Define df_agreement_cleaned FIRST\n",
    "df_agreement_cleaned = df_agreement_reset[df_agreement_reset['labels'].map(len) > 0].reset_index(drop = True) # Line 46 (moved up) - df_agreement_cleaned is DEFINED here FIRST\n",
    "\n",
    "# 불용 라벨 제거 (optional): ['nan', '', None] 라벨 제거\n",
    "labels_to_remove = ['nan', '', None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels(labels):\n",
    "    return [label for label in labels if label not in labels_to_remove and pd.notna(label) and label != 'nan']\n",
    "\n",
    "# Assign 'labels_cleaned' column to the ALREADY DEFINED df_agreement_cleaned\n",
    "df_agreement_cleaned['labels_cleaned'] = df_agreement_reset['labels'].apply(remove_labels) # Line 43 (moved down) - Assign to df_agreement_cleaned AFTER it's defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링된 데이터프레임\n",
    "df_cleaned = df[df[\"common_labels\"].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "texts = df_cleaned[\"본문\"].dropna().astype(str).tolist()\n",
    "authors = df_cleaned[\"저자\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_cleaned를 메타데이터에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 문장들을 Document 형태로 변환\n",
    "documents = [Document(page_content=text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    metadata: dict\n",
    "    page_content: str\n",
    "\n",
    "def add_common_labels_to_documents(documents: List[Document], df_cleaned, column_name1=\"emotion\", column_name2=\"author\"):\n",
    "    \"\"\"\n",
    "    documents의 metadata 딕셔너리에 df_cleaned의 'common_labels' 열 값을 추가하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): Document 객체 리스트.\n",
    "        df_cleaned (pd.DataFrame): 'common_labels' 열을 포함하는 데이터프레임.\n",
    "        column_name (str): 추가할 열 이름. 기본값은 'common_labels'.\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: metadata가 업데이트된 Document 객체 리스트.\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        if i < len(df_cleaned):\n",
    "            # df_cleaned의 'common_labels' 값을 metadata에 추가\n",
    "            doc.metadata[column_name1] = df_cleaned[\"common_labels\"].iloc[i]\n",
    "            doc.metadata[column_name2] = df_cleaned[\"저자\"].iloc[i]  # 'author' 열 추가\n",
    "        else:\n",
    "            # df_cleaned에 없는 경우 빈 리스트 추가\n",
    "            doc.metadata[column_name1] = []\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 호출\n",
    "updated_documents = add_common_labels_to_documents(documents, df_cleaned)\n",
    "\n",
    "# 결과 출력\n",
    "# page_content='죽는 날까지 하늘을 우러러' metadata={'common_labels': ['비장함']}\n",
    "# page_content='한 점 부끄럼이 없기를,' metadata={'common_labels': ['비장함', '부끄러움']}\n",
    "# page_content='잎새에 이는 바람에도' metadata={'common_labels': ['기대감']}\n",
    "# page_content='나는 괴로워했다.' metadata={'common_labels': ['패배/자기혐오', '절망', '슬픔', '힘듦/지침']}\n",
    "# page_content='별을 노래하는 마음으로' metadata={'common_labels': ['기쁨', '기대감', '아껴주는']}\n",
    "# page_content='모든 죽어가는 것을 사랑해야지' metadata={'common_labels': ['비장함', '불쌍함/연민', '아껴주는']}\n",
    "# page_content='걸어가야겠다.' metadata={'common_labels': ['비장함']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updated_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mupdated_documents\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updated_documents' is not defined"
     ]
    }
   ],
   "source": [
    "updated_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone : KcElectra 언어모델 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.embeddings.base import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KcELECTRAEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"beomi/KcELECTRA-base\", device: str = \"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self._embed(text).tolist() for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._embed(text).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3️⃣ 벡터 임베딩 모델 로딩 (한국어 지원하는 모델 권장) KcElectra -> backbone 모델로 사용\n",
    "embedding_model = KcELECTRAEmbeddings()\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sbert-sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ FAISS VectorDB 생성\n",
    "vectorstore = FAISS.from_documents(updated_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"../data/poetry_vectorstore\")  # 벡터 DB 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KcElectra + KPoEM Metadata VectorStore 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에서 로드 (신뢰할 수 있는 파일일 경우)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"../data/poetry_vectorstore\", \n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 시 생성 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ 전체 체인\n",
    "def emotion_to_poetry_V(user_input):\n",
    "    scores = classify_emotion(user_input)\n",
    "    # top_emotion = max(scores, key=scores.get)\n",
    "    # top_emotion = dict_to_text(scores)\n",
    "    # 감정 범위 지정\n",
    "    top_n = 10\n",
    "    sorted_emotions = sorted(scores, key=scores.get, reverse=True)\n",
    "    top_emotions = set(sorted_emotions[:top_n])\n",
    "    # 관련 시구 검색\n",
    "    context = vectorstore.similarity_search(user_input, k=20)  # k=10로 설정, 필요에 따라 조정 가능\n",
    "\n",
    "    # 감정 metadata를 기반으로 필터링 (optional)\n",
    "    filtered_results = [\n",
    "      doc for doc in context\n",
    "      if any(emotion in top_emotions for emotion in doc.metadata.get(\"emotion\", []))\n",
    "    ]\n",
    "\n",
    "    # 문맥 강화용 텍스트 추출\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in filtered_results])\n",
    "    # 프롬프트 생성\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### 시스템:\n",
    "    당신은 창의적이고 감성적인 근현대 시인입니다. 다음 감정목록에 언급된 감정들을 주된 시의 정서로 활용하세요. 감정어휘를 직접 언급하는 것은 피해주세요.\n",
    "    영어나 다른 언어는 사용하지 말고, 한국어로만 작성하는 것을 명심하세요.\n",
    "    {context_snippets} 이 문장들에서 옛스러운 한국 고유의 표현을 주로 사용하여 시를 하나 지어 주세요.\n",
    "    은유와 상징을 사용하여 창의적으로 감정을 표현하세요. 반복을 최대한 하지마세요.\n",
    "    감정 목록: {top_emotion}\n",
    "    ### 시:\n",
    "    \"\"\"\n",
    "    \n",
    "    # LangChain Prompt + LLM 실행\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    \n",
    "    # print(template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(top_emotion=top_emotions, context_snippets=context_text)\n",
    "    # 결과 출력\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9️⃣ 테스트 실행\n",
    "user_text = \"\"\"\n",
    "하루 종일 지친 몸으로만 떠돌다가\n",
    "땅에 떨어져 죽지 못한\n",
    "햇빛들은 줄지어 어디로 가는 걸까\n",
    "\n",
    "웅성웅성 가장 근심스러운 색깔로 서행하며\n",
    "이미 어둠이 깔리는 소각장으로 몰려들어\n",
    "몇 점 폐휴지로 타들어가는 오루 6시의 참혹한 형량\n",
    "단 한 번 후회도 용서하지 않는 무서운 시간\n",
    "바람은 긴 채찍을 휘둘러\n",
    "살아서 빛나는 온갖 상징을 몰아내고 있다.\n",
    "\n",
    "도시는 곧 활자들이 일제히 빠져 달아나\n",
    "속도 없이 페이지를 펄럭이는 텅 빈 한 권 책이 되리라.\n",
    "승부를 알 수 없는 하루와의 싸움에서\n",
    "우리는 패배했을까. 오늘도 물어보는 사소한 물음은\n",
    "그러나 우리의 일생을 텅텅 흔드는 것.\n",
    "\n",
    "오후 6시의 소각장 위로 말없이\n",
    "검은 연기가 우산처럼 펼쳐지고\n",
    "이젠 우리들의 차례였다.\n",
    "두렵지 않은가.\n",
    "밤이면 그림자를 빼앗겨 누구나 아득한 혼자였다.\n",
    "\n",
    "문득 거리를 빠르게 스쳐가는 일상의 공포\n",
    "보여다오. 지금까지 무엇을 했는가 살아 있는 그대여\n",
    "오후 6시 우리들 이마에도 아, 붉은 노을이 떴다.\n",
    "\n",
    "그러면 우리는 어디로 가지?\n",
    "아직도 펄펄 살아 있는 우리는 이제 각자 어디로 가지? \n",
    "\"\"\" # 기형도 - 노을\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎴 생성된 시:\n",
      " ### 시스템:\n",
      "    당신은 창의적이고 감성적인 근현대 시인입니다. 다음 감정목록에 언급된 감정들을 주된 시의 정서로 활용하세요. 감정어휘를 직접 언급하는 것은 피해주세요.\n",
      "    영어나 다른 언어는 사용하지 말고, 한국어로만 작성하는 것을 명심하세요.\n",
      "    홈싹홈싹 숨치우는 보드라운 모래 바닥과 같은 긴 길이, 항상 외롭고 힘없는 저의 발길을 그리운 당신한테로 인도하여 주겠지요.\n",
      "시내를 따라 구비친 모랫길이 어둠의 품에 안겨서 잠들 때에 나는 고요하고 아득한 하늘에 긴 한숨의 사라진 자취를 남기고 게으른 걸음으로 돌아옵니다\n",
      "살구나무 그늘로 얼굴을 가리고, 병원 뒤뜰에 누워, 젊은 여자가 흰 옷 아래로 하얀 다리를 드러내 놓고 일광욕을 한다. 한나절이 기울도록 가슴을 앓는 다는 이 여자를 찾아오는 이, 나비 한 마리도 없다. 슬프지도 않은 살구나무 가지에는 바람조차 없다.\n",
      "키가크고유쾌한수목이키작은자식을낳았다궤조가평편한곳에풍매식물의종자가떨어지지만냉담한배척이한결같아관목은초엽으로쇄약하고초엽은하향하고그밑에서청사는점점수척하여가고땀이흐르고머지않은곳에서수은이흔들리고숨어흐르는수맥에말뚝박는소리가들렸다\n",
      "「내가 그다지 사랑하든 그대여 내한평생(平生)에 차마 그대를 잊을수없소이다. 내차례에 못올사랑인줄은 알면서도 나혼자는 꾸준히생각하리다. 자그러면 내내어여쁘소서」\n",
      "담배연기의 한 무더기 그 실내에서 나는 긋지 아니한 성냥을 몇개비고 부러뜨렸다. 그 실내의 연기의 한 무더기 점화되어 나만 남기고 잘도 타나 보다 잉크는 축축하다 연필로 아무렇게나 시커먼 면을 그리면 연필은 종이 위에 흩어진다\n",
      "비낀 달빛이 이슬에 젖은 꽃수풀을 싸라기처럼 부시듯이 당신의 떠난 한(恨)은 드는 칼이 되어서 나의 애를 토막토막 끊어 놓았습니다\n",
      "내가 그 컵을 손으로 꼭 쥐었을 때 내 팔에서는 난데없는 팔 하나가 접목처럼 돋히더니 그 팔에 달린 손은 그 사기컵을 번쩍 들어 마룻바닥에 메어부딪는다.\n",
      "나비가 한마리 꽃밭에 날아 들다 그물에 걸리었다. 노—란 날개를 파득거려도 파득거려도 나비는 자꾸 감기우기만 한다. 거미가 쏜살같이 가더니 끝없는 끝없는 실을 뽑아 나비의 온몸을 감아 버린다. 사나이는 긴 한숨을 쉬었다.\n",
      "눈이 녹으면 남은 발자욱 자리마다 꽃이 피리니 꽃 사이로 발자욱을 찾아 나서면 일년 열두달 하냥 내 마음에는 눈이 내리리라.\n",
      "달빛이내등에묻은거적자국에앉으면내그림자에는실고추같은피가아물거리고대신혈관에는달빛에놀래인냉수가방울방울젖기로니너는내벽돌을씹어삼킨원통하게배고파이지러진헝겊심장을들여다보면서어항이라하느냐\n",
      "나의 노래는 처녀(處女)의 청춘(靑春)을 쥐어짜서 보기도 어려운 맑은 물을 만듭니다 이 문장들에서 옛스러운 한국 고유의 표현을 주로 사용하여 시를 하나 지어 주세요.\n",
      "    은유와 상징을 사용하여 창의적으로 감정을 표현하세요. 반복을 최대한 하지마세요.\n",
      "    감정 목록: {'서러움', '안타까움/실망', '슬픔', '비장함', '힘듦/지침', '불안/걱정', '당황/난처', '의심/불신', '부담/안_내킴', '공포/무서움'}\n",
      "    ### 시: \n",
      "        서쪽 태양이 오르며 불어오던 아침,\n",
      "            소금산 속에 깃발이 흔드는 풍경,\n",
      "                비행기를 맞이하기 위해 집 앞까지 도착했다.\n",
      "\n",
      "        낙동강변 광학단지가 멎게 빛나는 낮,\n",
      "            두 개의 미주교 대문에 배터리로 전환된 방,\n",
      "                    분명히 우리들의 고통들이 담긴 계기가 되었다.\n",
      "\n",
      "        해질녘이 와야 할 것 같는데도,\n",
      "             신선해지는 냇물을 통해 새끼사탕이라는 유령을 볼 수 있다.\n",
      "          그리고 밤이 될 때부터 다시 시작되는 생존전쟁,\n",
      "\n",
      "  \"내일은 새로운 희망\"이라고 간곡히 말했기에\n",
      "         하지만 지금 그 말을 듣는 사람들은 모두 죽어버렸다고 생각한다.\n",
      "\n",
      "\n",
      "        우체국의 편지를 받으며 세상을 바라본다면?\n",
      "           공중트레인은 산책하며 즐기는 아이들과 함께한 시간이었다\n",
      "\n",
      "       오늘날에도 많은 사람들이 이러한 상황을 겪고 있음을 깨닫게 하는 순간...\n",
      "              그러나 그에게는 아직 더 큰 thử验이 기다리는 것이 있었다\n",
      "\n",
      "\n",
      "      - : 네가 제대로 살아남아 있는 이유라고 묻게 해야しょう? \n",
      "\n",
      "     (네가),..\n",
      "     \n",
      "위 내용 중 where = {0} 부분은 원문을 변경할 경우에 해당될 것입니다. 여기서는 [ ] 형태로 수정했습니다.\n",
      "\n",
      "\n",
      "\n",
      "1. **제출:** 다음과 같이 modifying합니다:\n",
      "\n",
      "```\n",
      "서쪽 태阳升起时, 향 tưởng소 salted soil beneath my feet becomes a sea of sorrow and longing,\n",
      "   the wind whispers secrets in my ear as I wander through this desolate landscape...\n",
      "\n",
      "   Morning light creeps over the horizon like an old friend returning to our doorstep after years apart;\n",
      "               The waves gently lap against the shore with soothing melodies that bring solace for weary souls such as mine who roam these barren lands searching desperately hoping someday they might find peace....\n",
      "\n",
      "   As dusk falls on another day spent at home alone under starry skies we reminisce about memories past before drifting off into dreamland once again; yet deep within me there is still one nagging voice echoing silently reminding us never give up even when all seems lost forevermore – until tomorrow morning arrives bringing fresh hope anew!…\n",
      "\n",
      "  And so each dawn brings new beginnings while nightfall casts shadows upon them casting doubts whether life truly has meaning or if it's just\n"
     ]
    }
   ],
   "source": [
    "generated_poem_V = emotion_to_poetry_V(user_text)\n",
    "\n",
    "print(\"🎴 생성된 시:\\n\", generated_poem_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시 부분문 잘라서 보기\n",
    "\n",
    "def extract_poem(text: str) -> str:\n",
    "    start_marker = \"### 시:\"\n",
    "    if start_marker in text:\n",
    "        return text.split(start_marker, 1)[1].strip()\n",
    "    else:\n",
    "        return \"[시를 찾을 수 없습니다]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎴 생성된 시:\n",
      " 서쪽 태양이 오르며 불어오던 아침,\n",
      "            소금산 속에 깃발이 흔드는 풍경,\n",
      "                비행기를 맞이하기 위해 집 앞까지 도착했다.\n",
      "\n",
      "        낙동강변 광학단지가 멎게 빛나는 낮,\n",
      "            두 개의 미주교 대문에 배터리로 전환된 방,\n",
      "                    분명히 우리들의 고통들이 담긴 계기가 되었다.\n",
      "\n",
      "        해질녘이 와야 할 것 같는데도,\n",
      "             신선해지는 냇물을 통해 새끼사탕이라는 유령을 볼 수 있다.\n",
      "          그리고 밤이 될 때부터 다시 시작되는 생존전쟁,\n",
      "\n",
      "  \"내일은 새로운 희망\"이라고 간곡히 말했기에\n",
      "         하지만 지금 그 말을 듣는 사람들은 모두 죽어버렸다고 생각한다.\n",
      "\n",
      "\n",
      "        우체국의 편지를 받으며 세상을 바라본다면?\n",
      "           공중트레인은 산책하며 즐기는 아이들과 함께한 시간이었다\n",
      "\n",
      "       오늘날에도 많은 사람들이 이러한 상황을 겪고 있음을 깨닫게 하는 순간...\n",
      "              그러나 그에게는 아직 더 큰 thử验이 기다리는 것이 있었다\n",
      "\n",
      "\n",
      "      - : 네가 제대로 살아남아 있는 이유라고 묻게 해야しょう? \n",
      "\n",
      "     (네가),..\n",
      "     \n",
      "위 내용 중 where = {0} 부분은 원문을 변경할 경우에 해당될 것입니다. 여기서는 [ ] 형태로 수정했습니다.\n",
      "\n",
      "\n",
      "\n",
      "1. **제출:** 다음과 같이 modifying합니다:\n",
      "\n",
      "```\n",
      "서쪽 태阳升起时, 향 tưởng소 salted soil beneath my feet becomes a sea of sorrow and longing,\n",
      "   the wind whispers secrets in my ear as I wander through this desolate landscape...\n",
      "\n",
      "   Morning light creeps over the horizon like an old friend returning to our doorstep after years apart;\n",
      "               The waves gently lap against the shore with soothing melodies that bring solace for weary souls such as mine who roam these barren lands searching desperately hoping someday they might find peace....\n",
      "\n",
      "   As dusk falls on another day spent at home alone under starry skies we reminisce about memories past before drifting off into dreamland once again; yet deep within me there is still one nagging voice echoing silently reminding us never give up even when all seems lost forevermore – until tomorrow morning arrives bringing fresh hope anew!…\n",
      "\n",
      "  And so each dawn brings new beginnings while nightfall casts shadows upon them casting doubts whether life truly has meaning or if it's just\n"
     ]
    }
   ],
   "source": [
    "print(\"🎴 생성된 시:\\n\",extract_poem(generated_poem_V))  # 시 추출 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'공포/무서움': 0.33000001311302185,\n",
       " '당황/난처': 0.42899999022483826,\n",
       " '불안/걱정': 0.46299999952316284,\n",
       " '비장함': 0.3160000145435333,\n",
       " '슬픔': 0.3230000138282776,\n",
       " '힘듦/지침': 0.3070000112056732}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotion(\"\"\"서쪽 태양이 오르며 불어오던 아침,\n",
    "            소금산 속에 깃발이 흔드는 풍경,\n",
    "                비행기를 맞이하기 위해 집 앞까지 도착했다.\n",
    "\n",
    "        낙동강변 광학단지가 멎게 빛나는 낮,\n",
    "            두 개의 미주교 대문에 배터리로 전환된 방,\n",
    "                    분명히 우리들의 고통들이 담긴 계기가 되었다.\n",
    "\n",
    "        해질녘이 와야 할 것 같는데도,\n",
    "             신선해지는 냇물을 통해 새끼사탕이라는 유령을 볼 수 있다.\n",
    "          그리고 밤이 될 때부터 다시 시작되는 생존전쟁,\n",
    "\n",
    "  \"내일은 새로운 희망\"이라고 간곡히 말했기에\n",
    "         하지만 지금 그 말을 듣는 사람들은 모두 죽어버렸다고 생각한다.\n",
    "\n",
    "\n",
    "        우체국의 편지를 받으며 세상을 바라본다면?\n",
    "           공중트레인은 산책하며 즐기는 아이들과 함께한 시간이었다\n",
    "\n",
    "       오늘날에도 많은 사람들이 이러한 상황을 겪고 있음을 깨닫게 하는 순간...\n",
    "              그러나 그에게는 아직 더 큰 thử验이 기다리는 것이 있었다\n",
    "\n",
    "\n",
    "      - : 네가 제대로 살아남아 있는 이유라고 묻게 해야しょう? \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
