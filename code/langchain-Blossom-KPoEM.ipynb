{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.4.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.11.10)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m462.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, typing-inspection, typing-inspect, tenacity, python-dotenv, marshmallow, jsonpatch, httpx-sse, requests-toolbelt, dataclasses-json, pydantic-settings, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.25 langchain-community-0.3.25 langchain-core-0.3.65 langchain-text-splitters-0.3.8 langsmith-0.3.45 marshmallow-3.26.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "from transformers import ElectraModel, AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path (KOTE Finetuned): ./model/250127_KcElectra_kote.ckpt\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# KOTE íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ìˆ˜ì •)\n",
    "###########################\n",
    "best_ckpt_path_kote = './model/250127_KcElectra_kote.ckpt' # Colab ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
    "print(\"Best checkpoint path (KOTE Finetuned):\", best_ckpt_path_kote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOTETagger í´ë˜ìŠ¤ëŠ” ì´ì „ ì½”ë“œì™€ ë™ì¼\n",
    "class KOTETagger(pl.LightningModule): # KOTETagger í´ë˜ìŠ¤ ì •ì˜ (ì´ì „ ì½”ë“œì—ì„œ ë³µì‚¬)\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.electra = AutoModel.from_pretrained(MODEL_NAME, return_dict=True) # pretrained_electra ì œê±° ë° ì§ì ‘ ë¡œë“œ\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, 44) # num_labels=44 (KOTE ë¼ë²¨ ê°œìˆ˜)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "        return loss, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, _ = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY) # ê°€ì¤‘ì¹˜ ê°ì‡ \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "###########################\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\n",
    "    \"ê°ë™/ê°íƒ„\", \"ê²½ì•…\", \"ê³ ë§ˆì›€\", \"ê³µí¬/ë¬´ì„œì›€\", \"ê·€ì°®ìŒ\", \"ê¸°ëŒ€ê°\", \"ê¸°ì¨\", \"ê¹¨ë‹¬ìŒ\",\n",
    "    \"ë†€ëŒ\", \"ë‹¹í™©/ë‚œì²˜\", \"ë¶€ë„ëŸ¬ì›€\", \"ë¶€ë‹´/ì•ˆ_ë‚´í‚´\", \"ë¶ˆìŒí•¨/ì—°ë¯¼\", \"ë¶ˆì•ˆ/ê±±ì •\", \"ë¶ˆí‰/ë¶ˆë§Œ\",\n",
    "    \"ë¹„ì¥í•¨\", \"ë¿Œë“¯í•¨\", \"ì„œëŸ¬ì›€\", \"ìŠ¬í””\", \"ì‹ ê¸°í•¨/ê´€ì‹¬\", \"ì•„ê»´ì£¼ëŠ”\", \"ì•ˆì‹¬/ì‹ ë¢°\", \"ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§\",\n",
    "    \"ì–´ì´ì—†ìŒ\", \"ì—†ìŒ\", \"ì—­ê²¨ì›€/ì§•ê·¸ëŸ¬ì›€\", \"ìš°ì­ëŒ/ë¬´ì‹œí•¨\", \"ì˜ì‹¬/ë¶ˆì‹ \", \"ì¬ë¯¸ì—†ìŒ\", \"ì ˆë§\",\n",
    "    \"ì¡´ê²½\", \"ì£„ì±…ê°\", \"ì¦ê±°ì›€/ì‹ ë‚¨\", \"ì¦ì˜¤/í˜ì˜¤\", \"ì§€ê¸‹ì§€ê¸‹\", \"ì§œì¦\", \"íŒ¨ë°°/ìê¸°í˜ì˜¤\",\n",
    "    \"í¸ì•ˆ/ì¾Œì \", \"í•œì‹¬í•¨\", \"í–‰ë³µ\", \"í™”ë‚¨/ë¶„ë…¸\", \"í™˜ì˜/í˜¸ì˜\", \"íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)\", \"í˜ë“¦/ì§€ì¹¨\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kote_finetuned_model = KOTETagger.load_from_checkpoint(best_ckpt_path_kote)\n",
    "pretrained_electra = kote_finetuned_model.electra # ìˆ˜ì •: electra backboneë§Œ ê°€ì ¸ì˜´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# LightningModule ì •ì˜ (PoetryTagger) (ê¸°ì¡´ ì½”ë“œ í™œìš© + ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜, Dropout, Weight Decay, Learning Rate ê°ì†Œ, EarlyStopping patience ì¦ê°€)\n",
    "###########################\n",
    "INITIAL_LR = 1e-5 # í•™ìŠµë¥  ê°ì†Œ (ì›ë˜ 2e-5, 1e-5, 5e-6, 2e-6)\n",
    "DROPOUT_RATE = 0.5 # Dropout ë¹„ìœ¨ (0.1, 0.3, 0.5) - Dropout ì¶”ê°€\n",
    "WEIGHT_DECAY = 0.02 # Weight Decay ê°’ (0.001, 0.01, 0.02) - Weight Decay ì¶”ê°€\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "class PoetryTagger(pl.LightningModule):\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None, dropout_rate=DROPOUT_RATE): # dropout_rate hyperparameter\n",
    "        super().__init__()\n",
    "        self.electra = pretrained_electra # ìˆ˜ì •: KOTE íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ electra backbone ì‚¬ìš©\n",
    "        self.classifier = nn.Sequential( # nn.Sequential ì‚¬ìš©í•˜ì—¬ dropout layer ì¶”ê°€\n",
    "            nn.Linear(self.electra.config.hidden_size, len(emotion_labels)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ) # Classifier ì¶œë ¥ì¸µ í¬ê¸° ìë™ ì¡°ì •\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss() # ê¸°ë³¸ BCE Loss (ê°€ì¤‘ì¹˜ ë¯¸ì ìš©)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        # [CLS] í† í° ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "\n",
    "        return loss, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        loss, _ = self(input_ids, attention_mask, labels) # forward í•¨ìˆ˜ì— weights ì œê±°\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        loss, outputs = self(input_ids, attention_mask, labels) # validation lossëŠ” ê¸°ì¡´ BCE Loss ì‚¬ìš© (optional)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss} # validation metrics are optional\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY) # ê°€ì¤‘ì¹˜ ê°ì‡ \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# # Best checkpoint load & Evaluation (poetry-finetuning) (ê¸°ì¡´ ì½”ë“œ í™œìš©)\n",
    "# ###########################\n",
    "def get_latest_version_dir_poetry(base_dir=\"./lightning_logs/poetry-finetuning-3agreements-only\"): # poetry-weighted-finetuning ë¡œ ë³€ê²½\n",
    "    # version_0, version_1, ... version_50 ê²½ë¡œë¥¼ ëª¨ë‘ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ì—…\n",
    "    version_dirs = glob.glob(os.path.join(base_dir, \"version_*\"))\n",
    "    # ë²„ì „ ìˆ«ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "    version_dirs.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
    "    if not version_dirs:\n",
    "        raise FileNotFoundError(f\"No version_* directories found under '{base_dir}'\")\n",
    "    # ê°€ì¥ ë§ˆì§€ë§‰(ìˆ«ìê°€ ê°€ì¥ í°) ë²„ì „ ê²½ë¡œ ë°˜í™˜\n",
    "    return version_dirs[-1]\n",
    "\n",
    "def get_latest_checkpoint_poetry(version_dir):\n",
    "    ckpt_dir = os.path.join(version_dir, \"checkpoints\")\n",
    "    ckpt_list = glob.glob(os.path.join(ckpt_dir, \"*.ckpt\"))\n",
    "    ckpt_list.sort()  # íŒŒì¼ëª… ê¸°ì¤€ ì •ë ¬\n",
    "    if not ckpt_list:\n",
    "        raise FileNotFoundError(f\"No .ckpt found under '{ckpt_dir}'\")\n",
    "    # ê°€ì¥ ë§ˆì§€ë§‰ íŒŒì¼(ì •ë ¬ ê¸°ì¤€)\n",
    "    return ckpt_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path (Poetry Weighted Finetuned): ./lightning_logs/poetry-finetuning-3agreements-only/version_0/checkpoints/epoch9-val_loss0.2566.ckpt\n"
     ]
    }
   ],
   "source": [
    "# KPoEM ëª¨ë¸ ë¡œë“œ (local)\n",
    "latest_version_dir_poetry = get_latest_version_dir_poetry(\"./lightning_logs/poetry-finetuning-3agreements-only\") # poetry-weighted-finetuning ë¡œ ë³€ê²½\n",
    "best_ckpt_path_poetry = get_latest_checkpoint_poetry(latest_version_dir_poetry)\n",
    "print(\"Best checkpoint path (Poetry Weighted Finetuned):\", best_ckpt_path_poetry) # poetry-weighted-finetuning ë¡œ ë³€ê²½\n",
    "\n",
    "best_model_poetry = PoetryTagger.load_from_checkpoint(best_ckpt_path_poetry)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model_poetry.to(device)\n",
    "best_model_poetry.eval()\n",
    "best_model_poetry.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"í•˜ë£¨ ì¢…ì¼ ì§€ì¹œ ëª¸ìœ¼ë¡œë§Œ ë– ëŒë‹¤ê°€\n",
    "ë•…ì— ë–¨ì–´ì ¸ ì£½ì§€ ëª»í•œ\n",
    "í–‡ë¹›ë“¤ì€ ì¤„ì§€ì–´ ì–´ë””ë¡œ ê°€ëŠ” ê±¸ê¹Œ\n",
    "\n",
    "ì›…ì„±ì›…ì„± ê°€ì¥ ê·¼ì‹¬ìŠ¤ëŸ¬ìš´ ìƒ‰ê¹”ë¡œ ì„œí–‰í•˜ë©°\n",
    "ì´ë¯¸ ì–´ë‘ ì´ ê¹”ë¦¬ëŠ” ì†Œê°ì¥ìœ¼ë¡œ ëª°ë ¤ë“¤ì–´\n",
    "ëª‡ ì  ííœ´ì§€ë¡œ íƒ€ë“¤ì–´ê°€ëŠ” ì˜¤ë£¨ 6ì‹œì˜ ì°¸í˜¹í•œ í˜•ëŸ‰\n",
    "ë‹¨ í•œ ë²ˆ í›„íšŒë„ ìš©ì„œí•˜ì§€ ì•ŠëŠ” ë¬´ì„œìš´ ì‹œê°„\n",
    "ë°”ëŒì€ ê¸´ ì±„ì°ì„ íœ˜ë‘˜ëŸ¬\n",
    "ì‚´ì•„ì„œ ë¹›ë‚˜ëŠ” ì˜¨ê°– ìƒì§•ì„ ëª°ì•„ë‚´ê³  ìˆë‹¤.\n",
    "\n",
    "ë„ì‹œëŠ” ê³§ í™œìë“¤ì´ ì¼ì œíˆ ë¹ ì ¸ ë‹¬ì•„ë‚˜\n",
    "ì†ë„ ì—†ì´ í˜ì´ì§€ë¥¼ í„ëŸ­ì´ëŠ” í…… ë¹ˆ í•œ ê¶Œ ì±…ì´ ë˜ë¦¬ë¼.\n",
    "ìŠ¹ë¶€ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” í•˜ë£¨ì™€ì˜ ì‹¸ì›€ì—ì„œ\n",
    "ìš°ë¦¬ëŠ” íŒ¨ë°°í–ˆì„ê¹Œ. ì˜¤ëŠ˜ë„ ë¬¼ì–´ë³´ëŠ” ì‚¬ì†Œí•œ ë¬¼ìŒì€\n",
    "ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ì˜ ì¼ìƒì„ í……í…… í”ë“œëŠ” ê²ƒ.\n",
    "\n",
    "ì˜¤í›„ 6ì‹œì˜ ì†Œê°ì¥ ìœ„ë¡œ ë§ì—†ì´\n",
    "ê²€ì€ ì—°ê¸°ê°€ ìš°ì‚°ì²˜ëŸ¼ í¼ì³ì§€ê³ \n",
    "ì´ì   ìš°ë¦¬ë“¤ì˜ ì°¨ë¡€ì˜€ë‹¤.\n",
    "ë‘ë µì§€ ì•Šì€ê°€.\n",
    "ë°¤ì´ë©´ ê·¸ë¦¼ìë¥¼ ë¹¼ì•—ê²¨ ëˆ„êµ¬ë‚˜ ì•„ë“í•œ í˜¼ìì˜€ë‹¤.\n",
    "\n",
    "ë¬¸ë“ ê±°ë¦¬ë¥¼ ë¹ ë¥´ê²Œ ìŠ¤ì³ê°€ëŠ” ì¼ìƒì˜ ê³µí¬\n",
    "ë³´ì—¬ë‹¤ì˜¤. ì§€ê¸ˆê¹Œì§€ ë¬´ì—‡ì„ í–ˆëŠ”ê°€ ì‚´ì•„ ìˆëŠ” ê·¸ëŒ€ì—¬\n",
    "ì˜¤í›„ 6ì‹œ ìš°ë¦¬ë“¤ ì´ë§ˆì—ë„ ì•„, ë¶‰ì€ ë…¸ì„ì´ ë–´ë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë©´ ìš°ë¦¬ëŠ” ì–´ë””ë¡œ ê°€ì§€?\n",
    "ì•„ì§ë„ í„í„ ì‚´ì•„ ìˆëŠ” ìš°ë¦¬ëŠ” ì´ì œ ê°ì ì–´ë””ë¡œ ê°€ì§€?\n",
    "\"\"\" # ê¸°í˜•ë„ - ë…¸ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_emotion(sample_text):\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sample_text,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ì…ë ¥ í…ì„œ ë˜í•œ ê°™ì€ deviceë¡œ ì´ë™\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        # forward\n",
    "        _, predictions = best_model_poetry(input_ids, attention_mask)  # best_model_poetry ì‚¬ìš©\n",
    "\n",
    "    # ì¶”ë¡  ê²°ê³¼ë¥¼ CPUë¡œ ê°€ì ¸ì™€ numpyë¡œ ë³€í™˜\n",
    "    predictions = predictions.flatten().cpu().numpy()\n",
    "\n",
    "    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥ (ìˆ«ìê°’ìœ¼ë¡œ ë³€í™˜)\n",
    "    result_dict = {\n",
    "        label_name: float(round(score, 3))  # np.float32 -> float ë³€í™˜\n",
    "        for label_name, score in zip(emotion_labels, predictions)\n",
    "        if score > THRESHOLD\n",
    "    }\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    # print(\"\\n[Sample Inference ê²°ê³¼]\")\n",
    "    # print(result_dict)\n",
    "\n",
    "    return result_dict\n",
    "    # ì˜ˆì‹œ ì¶œë ¥\n",
    "    # {'ë¶ˆì•ˆ/ê±±ì •': 0.336, 'ìŠ¬í””': 0.311}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample Inference ê²°ê³¼]\n",
      "{'ê³µí¬/ë¬´ì„œì›€': 0.42800000309944153, 'ë†€ëŒ': 0.30399999022483826, 'ë‹¹í™©/ë‚œì²˜': 0.5070000290870667, 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´': 0.39500001072883606, 'ë¶ˆì•ˆ/ê±±ì •': 0.550000011920929, 'ë¹„ì¥í•¨': 0.3319999873638153, 'ì„œëŸ¬ì›€': 0.35199999809265137, 'ìŠ¬í””': 0.41499999165534973, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.30799999833106995, 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§': 0.3070000112056732, 'ì˜ì‹¬/ë¶ˆì‹ ': 0.30399999022483826, 'í˜ë“¦/ì§€ì¹¨': 0.33799999952316284}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ê³µí¬/ë¬´ì„œì›€': 0.42800000309944153,\n",
       " 'ë†€ëŒ': 0.30399999022483826,\n",
       " 'ë‹¹í™©/ë‚œì²˜': 0.5070000290870667,\n",
       " 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´': 0.39500001072883606,\n",
       " 'ë¶ˆì•ˆ/ê±±ì •': 0.550000011920929,\n",
       " 'ë¹„ì¥í•¨': 0.3319999873638153,\n",
       " 'ì„œëŸ¬ì›€': 0.35199999809265137,\n",
       " 'ìŠ¬í””': 0.41499999165534973,\n",
       " 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.30799999833106995,\n",
       " 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§': 0.3070000112056732,\n",
       " 'ì˜ì‹¬/ë¶ˆì‹ ': 0.30399999022483826,\n",
       " 'í˜ë“¦/ì§€ì¹¨': 0.33799999952316284}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotion(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bllossom ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer_bllossom = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer_bllossom.pad_token = tokenizer_bllossom.eos_token  # Blossomì€ pad_tokenì´ ì—†ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b888052c7084ecf8b59fa8d826bcfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # ë˜ëŠ” \"auto\"\n",
    ")\n",
    "\n",
    "# 2. í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_bllossom,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# 3. LangChainìš© LLM ë˜í¼\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "template = \"\"\"\n",
    "### ì‹œìŠ¤í…œ:\n",
    "ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹œì ì¸ í•œêµ­ì–´ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ì„ í‘œí˜„í•œ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\n",
    "\n",
    "### ê°ì •: {emotion}\n",
    "### ì‹œ:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"emotion\"],\n",
    "    template=template.strip()\n",
    ")\n",
    "# 5. LLMChain êµ¬ì„±\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/1771877612.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(\"ìŠ¬í””\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ì‹œìŠ¤í…œ:\n",
      "ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹œì ì¸ í•œêµ­ì–´ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ì„ í‘œí˜„í•œ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\n",
      "\n",
      "### ê°ì •: ìŠ¬í””\n",
      "### ì‹œ: 'ê·¸ë¦¬ìš´ ì‹œê°„'\n",
      "\n",
      "ì—¬ëŸ¬ ë²ˆì˜ ì•„ë¦„ë‹¤ìš´ ì‹œê°„ë“¤ì´ ì§€ë‚¬ìŠµë‹ˆë‹¤.\n",
      "ë‚´ ë§ˆìŒì†ì— ë‚¨ì•„ ìˆëŠ” ê²ƒì€ ìŠ¬í””ë¿.\n",
      "\n",
      "ê·¸ê³³ì—ì„œ ë‚˜ëŠ” ê·¸ë“¤ì„ ê¸°ë‹¤ë ¸ì§€ë§Œ,\n",
      "ëª¨ë‘ê°€ ë– ë‚œ í›„, ë‚˜ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë‚˜ë„ ê·¸ë“¤ì€ ë‚˜ë¥¼ ê¸°ë‹¤ë ¸ì§€ë§Œ,\n",
      "ë§ˆì¹¨ë‚´ëŠ” ë‚˜ë§Œì„ ì°¾ì•˜ë‹¤ê³  ë§í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì—¬ê¸°ì„œ ë‚˜ëŠ” í˜¼ìì„œ ë‚´ ê³ í†µì„ ëŠë¼ë©°,\n",
      "ì‹œì›í•œ ë°”ëŒê³¼ í•¨ê»˜ ìŠ¬í””ì„ ë‹´ì•„ëƒˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë‚˜ëŠ” ê·¸ë“¤ì—ê²Œ í•œ ë§ˆë””ë¥¼ à¤•à¤¹à¤¨ Ñ…Ğ¾Ñ‡Ñƒ.\n",
      "\"ë‚´ ë§ˆìŒ ì†ì—ëŠ” ë„ˆí¬ê°€ ì—†ìŒì„ ëŠë¼ëŠ” ë•Œê¹Œì§€,\n",
      "ë‚´ ê³ í†µì´æ°¸é í•˜ì§€ ì•Šì„ ê²ƒì´ë¼ê³ .\"\n",
      "\n",
      "ê·¸ë¦¬ê³  ë‚˜ëŠ” ê·¸ë“¤ì˜ ìë¦¬ë¥¼ ì°¨ì§€í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
      "ê·¸ë“¤ì²˜ëŸ¼ ì†Œì¤‘í•œ ì‹œê°„ì„ ë³´ë‚´ë©´,\n",
      "ë‚´ ë§ˆìŒì†ì—ëŠ” ë‹¤ì‹œ ì‚¬ë‘ê³¼ í–‰ë³µì´ ìƒê¸¸ ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ë¬¼ë¡ ì´ ì•„ë‹ˆì§€ë§Œ, ë‚˜ëŠ” ê·¸ë“¤ì²˜ëŸ¼,\n",
      "ìš°ë¦¬ ëª¨ë‘ê°€ ê²°êµ­ ì†Œì¤‘í•¨ì„ ì•Œê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë‚´ ê³ í†µì€ ê³„ì†ë  ê²ƒì…ë‹ˆë‹¤.\n",
      "í•˜ì§€ë§Œ ê·¸ ê³ í†µì´ ë‚´ ë§ˆìŒì†ì— ë‚¨ì•„ ìˆëŠ” ìŠ¬í””ì„ ë®ëŠ” ê²ƒì¼ ë¿ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë‚˜ëŠ” ê·¸ë“¤ì„ ìƒì—ˆì§€ë§Œ,\n",
      "ë‚´ ë§ˆìŒì†ì—ëŠ” ê·¸ë“¤ì˜ ì¡´ì¬ê°€ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ë“¤ì€ ë‚´ ê³ í†µì„ ë¤ë¶ˆë¤ ë˜ì ¸ì£¼ì—ˆê³ ,\n",
      "ë‚´ ë§ˆìŒì†ì— ìŠ¬í””ì„ ë‚¨ê²¼ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŸ¬ë‚˜ ìŠ¬í””ì€ ê²°êµ­ ìš°ë¦¬ì—ê²ŒëŠ” í° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
      "ê·¸ë“¤ì„ ìƒì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ ,\n",
      "ë‚´ ë§ˆìŒì†ì—ëŠ” ê·¸ë“¤ì˜ ì¡´ì¬ê°€ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë‚˜ëŠ” ê·¸ë“¤ì—ê²Œ í•œ ë§ˆë””ë¥¼ à¤•à¤¹à¤¨ Ñ…Ğ¾Ñ‡Ñƒ.\n",
      "\"ë‚´ ë§ˆìŒì†ì—ëŠ” ë„ˆí¬ê°€ ì—†ìŒì„ ëŠë¼ëŠ” ë•Œê¹Œì§€,\n",
      "ë‚´ ê³ í†µì´ æ°¸è¿œí•˜ì§€ ì•Šì„ ê²ƒì´ë¼ê³ .\"\n",
      "\n",
      "ê·¸ë¦¬ê³  ë‚˜ëŠ” ê·¸ë“¤ì˜ ìë¦¬ë¥¼ ì°¨ì§€í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
      "ê·¸ë“¤ì²˜ëŸ¼ ì†Œì¤‘í•œ ì‹œê°„ì„ ë³´ë‚´ë©´,\n",
      "ë‚´ ë§ˆìŒì†ì—ëŠ” ë‹¤ì‹œ ì‚¬ë‘ê³¼ í–‰ë³µì´ ìƒê¸¸ ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ë¬¼ë¡ ì´ ì•„ë‹ˆì§€ë§Œ, ë‚˜ëŠ” ê·¸ë“¤ì²˜ëŸ¼,\n",
      "ìš°ë¦¬ ëª¨ë‘ê°€ ê²°êµ­ ì†Œì¤‘í•¨ì„ ì•Œê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë‚´ ê³ í†µì€ ê³„ì†ë  ê²ƒì…ë‹ˆë‹¤.\n",
      "í•˜ì§€ë§Œ ê·¸ ê³ í†µì´ ë‚´ ë§ˆìŒì†ì— ë‚¨ì•„ ìˆëŠ” ìŠ¬í””ì„ ë®ëŠ” ê²ƒì¼ ë¿ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë‚˜ëŠ” ê·¸ë“¤ì„ ìƒì—ˆì§€ë§Œ,\n",
      "ë‚´ ë§ˆìŒì†ì—ëŠ” ê·¸ë“¤ì˜ ì¡´ì¬ê°€ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ë“¤ì€ ë‚´ ê³ í†µì„ ë¤\n"
     ]
    }
   ],
   "source": [
    "# 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ê¸°ë³¸ ì‹œ ìƒì„±í™•ì¸ \n",
    "result = chain.run(\"ìŠ¬í””\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°ì • ë¶„ë¥˜ ëª¨ë¸ ì ìš©í•˜ì—¬ Blossomìœ¼ë¡œ ì‹œ ìƒì„±(Vector DB ë¯¸ì ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¬ 2. Blossom Prompt Template\n",
    "poetry_template = \"\"\"\n",
    "### ì‹œìŠ¤í…œ:\n",
    "ë‹¹ì‹ ì€ ê°ì • ë¶„ì„ ê²°ê³¼ì— ê¸°ë°˜í•´ ì‹œë¥¼ ì°½ì‘í•˜ëŠ” í•œêµ­ì–´ ì‹œì¸ì…ë‹ˆë‹¤.\n",
    "ë‹¤ìŒ ê°ì • ë¶„í¬ë¥¼ ì°¸ê³ í•˜ì—¬ ì‹œë¥¼ í•œ í¸ ì§€ì–´ì£¼ì„¸ìš”.\n",
    "\n",
    "### ê°ì • ë¶„í¬:\n",
    "{emotion_prompt}\n",
    "\n",
    "### ì‹œ:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poetry_prompt = PromptTemplate(input_variables=[\"emotion_prompt\"], template=poetry_template.strip())\n",
    "poetry_chain = LLMChain(llm=llm, prompt=poetry_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ ê°ì • ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def generate_prompt(emotion_scores):\n",
    "    top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "    return f\"\"\"ë‹¹ì‹ ì€ ê°ì •ì´ ì„¬ì„¸í•œ í•œêµ­ í˜„ëŒ€ì‹œ ì‘ê°€ì…ë‹ˆë‹¤. \n",
    "'{top_emotion}'ì˜ ê°ì •ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì§§ì€ ì‹œë¥¼ í•œ í¸ ì°½ì‘í•´ ì£¼ì„¸ìš”.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4ï¸âƒ£ LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±\n",
    "# prompt = PromptTemplate.from_template(\"{emotion_prompt}\")\n",
    "# 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "template = \"\"\"\n",
    "    ### ì‹œìŠ¤í…œ:\n",
    "    ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹œì ì¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ëª©ë¡ ì •ë„ë¥¼ ë…¹ì—¬ë‚¸ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\n",
    "\n",
    "    ### ê°ì • ëª©ë¡: {emotion}\n",
    "    ### ì‹œ:\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"emotion\"],\n",
    "    template=template.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(d):\n",
    "    return '\\n'.join([f\"{k}: {v}\" for k, v in d.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poetry_section(template):\n",
    "    # Split the template by \"### ì‹œ:\" and extract the part after it\n",
    "    if \"### ì‹œ:\" in template:\n",
    "        poetry_section = template.split(\"### ì‹œ:\")[1].strip()\n",
    "        # Split by lines and return as a list\n",
    "        poetry_lines = poetry_section.splitlines()\n",
    "        return poetry_lines\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5ï¸âƒ£ ì „ì²´ íë¦„ í•¨ìˆ˜\n",
    "def emotion_to_poetry(sample_text): #sample text\n",
    "    emotion_scores = classify_emotion(sample_text)\n",
    "    # emotion_prompt = generate_prompt(emotion_scores)\n",
    "    # poem = chain.run(emotion_prompt=emotion_prompt)\n",
    "    # ê°ì • ë”•ì…”ë„ˆë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    emotion_text = dict_to_text(emotion_scores)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### ì‹œìŠ¤í…œ:\n",
    "    ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹œì ì¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ëª©ë¡ê³¼ ì •ë„ë¥¼ ë…¹ì—¬ë‚¸ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\n",
    "    ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”.\n",
    "    \n",
    "    ìƒì„±í•œ ì‹œë§Œ ì•Œë ¤ì£¼ì„¸ìš”. ê·¸ ì™¸ì— ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    ### ê°ì • ëª©ë¡: {emotion}\n",
    "    ### ì‹œ:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(emotion=emotion_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"í•˜ë£¨ ì¢…ì¼ ì§€ì¹œ ëª¸ìœ¼ë¡œë§Œ ë– ëŒë‹¤ê°€\n",
    "ë•…ì— ë–¨ì–´ì ¸ ì£½ì§€ ëª»í•œ\n",
    "í–‡ë¹›ë“¤ì€ ì¤„ì§€ì–´ ì–´ë””ë¡œ ê°€ëŠ” ê±¸ê¹Œ\n",
    "\n",
    "ì›…ì„±ì›…ì„± ê°€ì¥ ê·¼ì‹¬ìŠ¤ëŸ¬ìš´ ìƒ‰ê¹”ë¡œ ì„œí–‰í•˜ë©°\n",
    "ì´ë¯¸ ì–´ë‘ ì´ ê¹”ë¦¬ëŠ” ì†Œê°ì¥ìœ¼ë¡œ ëª°ë ¤ë“¤ì–´\n",
    "ëª‡ ì  ííœ´ì§€ë¡œ íƒ€ë“¤ì–´ê°€ëŠ” ì˜¤ë£¨ 6ì‹œì˜ ì°¸í˜¹í•œ í˜•ëŸ‰\n",
    "ë‹¨ í•œ ë²ˆ í›„íšŒë„ ìš©ì„œí•˜ì§€ ì•ŠëŠ” ë¬´ì„œìš´ ì‹œê°„\n",
    "ë°”ëŒì€ ê¸´ ì±„ì°ì„ íœ˜ë‘˜ëŸ¬\n",
    "ì‚´ì•„ì„œ ë¹›ë‚˜ëŠ” ì˜¨ê°– ìƒì§•ì„ ëª°ì•„ë‚´ê³  ìˆë‹¤.\n",
    "\n",
    "ë„ì‹œëŠ” ê³§ í™œìë“¤ì´ ì¼ì œíˆ ë¹ ì ¸ ë‹¬ì•„ë‚˜\n",
    "ì†ë„ ì—†ì´ í˜ì´ì§€ë¥¼ í„ëŸ­ì´ëŠ” í…… ë¹ˆ í•œ ê¶Œ ì±…ì´ ë˜ë¦¬ë¼.\n",
    "ìŠ¹ë¶€ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” í•˜ë£¨ì™€ì˜ ì‹¸ì›€ì—ì„œ\n",
    "ìš°ë¦¬ëŠ” íŒ¨ë°°í–ˆì„ê¹Œ. ì˜¤ëŠ˜ë„ ë¬¼ì–´ë³´ëŠ” ì‚¬ì†Œí•œ ë¬¼ìŒì€\n",
    "ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ì˜ ì¼ìƒì„ í……í…… í”ë“œëŠ” ê²ƒ.\n",
    "\n",
    "ì˜¤í›„ 6ì‹œì˜ ì†Œê°ì¥ ìœ„ë¡œ ë§ì—†ì´\n",
    "ê²€ì€ ì—°ê¸°ê°€ ìš°ì‚°ì²˜ëŸ¼ í¼ì³ì§€ê³ \n",
    "ì´ì   ìš°ë¦¬ë“¤ì˜ ì°¨ë¡€ì˜€ë‹¤.\n",
    "ë‘ë µì§€ ì•Šì€ê°€.\n",
    "ë°¤ì´ë©´ ê·¸ë¦¼ìë¥¼ ë¹¼ì•—ê²¨ ëˆ„êµ¬ë‚˜ ì•„ë“í•œ í˜¼ìì˜€ë‹¤.\n",
    "\n",
    "ë¬¸ë“ ê±°ë¦¬ë¥¼ ë¹ ë¥´ê²Œ ìŠ¤ì³ê°€ëŠ” ì¼ìƒì˜ ê³µí¬\n",
    "ë³´ì—¬ë‹¤ì˜¤. ì§€ê¸ˆê¹Œì§€ ë¬´ì—‡ì„ í–ˆëŠ”ê°€ ì‚´ì•„ ìˆëŠ” ê·¸ëŒ€ì—¬\n",
    "ì˜¤í›„ 6ì‹œ ìš°ë¦¬ë“¤ ì´ë§ˆì—ë„ ì•„, ë¶‰ì€ ë…¸ì„ì´ ë–´ë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë©´ ìš°ë¦¬ëŠ” ì–´ë””ë¡œ ê°€ì§€?\n",
    "ì•„ì§ë„ í„í„ ì‚´ì•„ ìˆëŠ” ìš°ë¦¬ëŠ” ì´ì œ ê°ì ì–´ë””ë¡œ ê°€ì§€?\n",
    "\"\"\" # ê¸°í˜•ë„ - ë…¸ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample Inference ê²°ê³¼]\n",
      "{'ê³µí¬/ë¬´ì„œì›€': 0.42800000309944153, 'ë†€ëŒ': 0.30399999022483826, 'ë‹¹í™©/ë‚œì²˜': 0.5070000290870667, 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´': 0.39500001072883606, 'ë¶ˆì•ˆ/ê±±ì •': 0.550000011920929, 'ë¹„ì¥í•¨': 0.3319999873638153, 'ì„œëŸ¬ì›€': 0.35199999809265137, 'ìŠ¬í””': 0.41499999165534973, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.30799999833106995, 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§': 0.3070000112056732, 'ì˜ì‹¬/ë¶ˆì‹ ': 0.30399999022483826, 'í˜ë“¦/ì§€ì¹¨': 0.33799999952316284}\n"
     ]
    }
   ],
   "source": [
    "# # 6ï¸âƒ£ í…ŒìŠ¤íŠ¸\n",
    "generated_poem = emotion_to_poetry(sample_text)\n",
    "\n",
    "# print(\"ğŸ´ ìƒì„±ëœ ì‹œ:\\n\")\n",
    "# print(generated_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ´ ìƒì„±ëœ ì‹œ:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ì–´ë‘  ì†ì—ì„œ ê¹Šì€ ëŒ€ì§€ë¥¼ ê¹Šì´ ë“¤ì–´ê°€ëŠ” ê²ƒ ê°™ì•„,',\n",
       " '    ë¬´ì–¸ê°€ì˜ ì†ê¸¸ì´ ëŠê»´ì§€ëŠ” ê³³ì— ì„œ ìˆë‹¤ë©´,',\n",
       " '    ë‘ë ¤ì›€ì´ ê¹Šì´ ë“¤ì–´ì˜¤ëŠ” ê²ƒ ê°™ì•„,',\n",
       " '    ë‚´ ë§ˆìŒ ì†ì— ë‘ë ¤ì›€ì´ ê¹Šì´ ë“¤ì–´ì˜¤ë‹¤ë‹ˆ,',\n",
       " '    ì´ê³³ì— ìˆëŠ” ëª¨ë“  ê²ƒì´ ë¬´ì„œìš´ ê²ƒ ê°™ì•„,',\n",
       " '    ë¬´ì–¸ê°€ê°€ ë‚˜ë¥¼ ì°¾ì•„ì˜¤ê³  ì‹¶ë‹¤ëŠ” ëŠë‚Œ,',\n",
       " '    ë‚´ ë§ˆìŒì†ì— ë¶ˆì•ˆí•œ ê³ ì–‘ì´ê°€ ë›°ì–´ë‹¤ë‹ˆë‹¤ë‹ˆ,',\n",
       " '    ì–´ë‘  ì†ì—ì„œ ë¬´ì–¸ê°€ê°€ ë‚˜ì™€ì„œë„ í•  ìˆ˜ ì—†ëŠ” ê²ƒ ê°™ì•„,',\n",
       " '    ë¬´ì–¸ê°€ê°€ ë‚˜ë¥¼ ì°¾ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•˜ë‹¤ë‹ˆ,',\n",
       " '    ì´ê³³ì— ìˆëŠ” ëª¨ë“  ê²ƒì´ ë¬´ì„œìš´ ê²ƒ ê°™ì•„,',\n",
       " '    ë¬´ì–¸ê°€ê°€ ë‚˜ë¥¼ ì°¾ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•˜ë‹¤ë‹ˆ,',\n",
       " '    ì–´ë‘  ì†ì—ì„œ ê¹Šì€ ëŒ€ì§€ë¥¼ ê¹Šì´ ë“¤ì–´ê°€ëŠ” ê²ƒ ê°™ì•„. ',\n",
       " '',\n",
       " '    (ì‹œê°€ ê¸¸ê²Œ ì´ì–´ì§€ë©´, ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ í†µí•´ ê°ì •ì´ ê¹Šì´ ë“¤ì–´ì˜¤ë©°, ì‹œê°€ ëë‚˜ë©´ ë‹¤ì‹œ ì‹œì‘ë©ë‹ˆë‹¤.)  ### 1ì°¨ ë¶„ì„:',\n",
       " '    1. ì‹œëŠ” ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ëŒ€ì‹ , ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ê°ì •ì„ í‘œí˜„í•˜ì˜€ìŠµë‹ˆë‹¤.',\n",
       " '    2. ì‹œëŠ” ì–´ë‘  ì†ì—ì„œì˜ ìƒí™©ì„ ì£¼ì œë¡œ í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ í†µí•´ ê°ì •ì´ ê¹Šì´ ë“¤ì–´ì˜¤ê²Œ í•©ë‹ˆë‹¤.',\n",
       " '    3. ì‹œëŠ” ê³„ì†í•´ì„œ ê°ì •ì„ ë°˜ë³µí•˜ë©°, ì‹œê°€ ëë‚˜ë©´ ë‹¤ì‹œ ì‹œì‘ë©ë‹ˆë‹¤. ì´ëŠ” ê°ì •ì´ ì§€ì†ì ìœ¼ë¡œ ê¹Šì´ ë“¤ì–´ì˜¤ëŠ” íš¨ê³¼ë¥¼ ì¤„ì…ë‹ˆë‹¤. ',\n",
       " '',\n",
       " '    ### 2ì°¨ ë¶„ì„:',\n",
       " \"    1. ì‹œëŠ” ê°ì •ì˜ ê¹Šì´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ 'ì–´ë‘  ì†ì—ì„œ'ë¼ëŠ” ìƒì§•ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” ê°ì •ì˜ ê¹Šì´ì™€ ì–´ë‘ìš´ ê°ì •ì„ ì—°ìƒì‹œí‚¤ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\",\n",
       " \"    2. 'ë‘ë ¤ì›€ì´ ê¹Šì´ ë“¤ì–´ì˜¤ëŠ” ê²ƒ ê°™ì•„'ë¼ëŠ” ë¬¸ì¥ì€ ê°ì •ì˜ ê¹Šì´ë¥¼ ê°•ì¡°í•˜ì˜€ìœ¼ë©°, 'ë¬´ì–¸ê°€ì˜ ì†ê¸¸ì´ ëŠê»´ì§€ëŠ” ê³³ì— ì„œ ìˆë‹¤ë©´'ì´ë¼ëŠ” ë¬¸ì¥ì€ ê°ì •ì˜ ì›ì²œì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\",\n",
       " \"    3. 'ë¬´ì–¸ê°€ê°€ ë‚˜ë¥¼ ì°¾ì•„ì˜¤ê³ \"]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ğŸ´ ìƒì„±ëœ ì‹œ:\\n\")\n",
    "extract_poetry_section(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë²¡í„° DB ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# ê·¼í˜„ëŒ€ì‹œ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ í™œìš© + ì¼ì¹˜ ë¼ë²¨ë§Œ ì‚¬ìš©)\n",
    "############################ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° (ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •)\n",
    "df = pd.read_csv(\"../data/á„á…©á†¼á„’á…¡á†¸á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º_0601_5á„‹á…µá†« - á„’á…¢á†¼á„ƒá…¡á†«á„‹á…±.csv\") # Colab ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •\n",
    "\n",
    "# ê°ì • ë¼ë²¨ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def labels_to_list(labels_str):\n",
    "    if pd.isna(labels_str):\n",
    "        return []\n",
    "    return [label.strip() for label in labels_str.split(',')]\n",
    "\n",
    "# ë¼ë²¨ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "df['annotator01_label_list'] = df['annotator01'].apply(labels_to_list)\n",
    "df['annotator02_label_list'] = df['annotator02'].apply(labels_to_list)\n",
    "df['annotator03_label_list'] = df['annotator03'].apply(labels_to_list)\n",
    "df['annotator04_label_list'] = df['annotator04'].apply(labels_to_list)\n",
    "df['annotator05_label_list'] = df['annotator05'].apply(labels_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_agreed_by_at_least_k(row, k=3):\n",
    "    \"\"\"\n",
    "    ê° í–‰(row)ì— ëŒ€í•´, ìµœì†Œ këª… ì´ìƒì´ ë™ì˜í•œ ê°ì •ë§Œ ì¶”ì¶œ\n",
    "\n",
    "    Parameters:\n",
    "    - row: annotator label listë“¤ì´ ìˆëŠ” DataFrame row\n",
    "    - k: ë™ì˜í•œ annotator ìµœì†Œ ìˆ˜ (ê¸°ë³¸ 2ëª…)\n",
    "\n",
    "    Returns:\n",
    "    - ê°ì • ë¦¬ìŠ¤íŠ¸ ì¤‘ këª… ì´ìƒì´ ê³µí†µìœ¼ë¡œ ì„ íƒí•œ ê°ì • ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    all_labels = (\n",
    "        row['annotator01_label_list'] +\n",
    "        row['annotator02_label_list'] +\n",
    "        row['annotator03_label_list'] +\n",
    "        row['annotator04_label_list'] +\n",
    "        row['annotator05_label_list']\n",
    "    )\n",
    "    counter = pd.Series(all_labels).value_counts() # ê°ì •ë³„ ê°œìˆ˜ ì„¸ê¸°\n",
    "    return [label for label, count in counter.items() if count >= k] # këª… ì´ìƒì´ ë™ì˜í•œ ê°ì • ë¦¬ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_agreed_by_at_least_k(row, k=3):\n",
    "    \"\"\"\n",
    "    ê° í–‰(row)ì— ëŒ€í•´, ìµœì†Œ këª… ì´ìƒì´ ë™ì˜í•œ ê°ì •ë§Œ ì¶”ì¶œ\n",
    "\n",
    "    Parameters:\n",
    "    - row: annotator label listë“¤ì´ ìˆëŠ” DataFrame row\n",
    "    - k: ë™ì˜í•œ annotator ìµœì†Œ ìˆ˜ (ê¸°ë³¸ 2ëª…)\n",
    "\n",
    "    Returns:\n",
    "    - ê°ì • ë¦¬ìŠ¤íŠ¸ ì¤‘ këª… ì´ìƒì´ ê³µí†µìœ¼ë¡œ ì„ íƒí•œ ê°ì • ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    all_labels = (\n",
    "        row['annotator01_label_list'] +\n",
    "        row['annotator02_label_list'] +\n",
    "        row['annotator03_label_list'] +\n",
    "        row['annotator04_label_list'] +\n",
    "        row['annotator05_label_list']\n",
    "    )\n",
    "    counter = pd.Series(all_labels).value_counts() # ê°ì •ë³„ ê°œìˆ˜ ì„¸ê¸°\n",
    "    return [label for label, count in counter.items() if count >= k] # këª… ì´ìƒì´ ë™ì˜í•œ ê°ì • ë¦¬ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ë¹„ì¥í•¨]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ë¹„ì¥í•¨, ë¶€ë„ëŸ¬ì›€]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ê¸°ëŒ€ê°]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[íŒ¨ë°°/ìê¸°í˜ì˜¤, ì ˆë§, ìŠ¬í””, í˜ë“¦/ì§€ì¹¨]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ê¸°ì¨, ê¸°ëŒ€ê°, ì•„ê»´ì£¼ëŠ”]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ë¹„ì¥í•¨, ë¶ˆìŒí•¨/ì—°ë¯¼, ì•„ê»´ì£¼ëŠ”]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ë¹„ì¥í•¨]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ê¸°ëŒ€ê°, ê°ë™/ê°íƒ„]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ìŠ¬í””]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              common_labels\n",
       "0                     [ë¹„ì¥í•¨]\n",
       "1               [ë¹„ì¥í•¨, ë¶€ë„ëŸ¬ì›€]\n",
       "2                     [ê¸°ëŒ€ê°]\n",
       "3  [íŒ¨ë°°/ìê¸°í˜ì˜¤, ì ˆë§, ìŠ¬í””, í˜ë“¦/ì§€ì¹¨]\n",
       "4           [ê¸°ì¨, ê¸°ëŒ€ê°, ì•„ê»´ì£¼ëŠ”]\n",
       "5       [ë¹„ì¥í•¨, ë¶ˆìŒí•¨/ì—°ë¯¼, ì•„ê»´ì£¼ëŠ”]\n",
       "6                        []\n",
       "7                     [ë¹„ì¥í•¨]\n",
       "8              [ê¸°ëŒ€ê°, ê°ë™/ê°íƒ„]\n",
       "9                      [ìŠ¬í””]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2ëª… ì´ìƒ ë™ì˜í•œ ê°ì • ë¦¬ìŠ¤íŠ¸ë¡œ ìƒˆ ì»¬ëŸ¼ ìƒì„±\n",
    "df['common_labels'] = df.apply(lambda row: get_labels_agreed_by_at_least_k(row, k=3), axis=1)\n",
    "\n",
    "df[['common_labels']].head(10) # 3ëª… ì´ìƒ ë™ì˜í•œ ê°ì • ë¦¬ìŠ¤íŠ¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/3686308332.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_agreement['labels'] = df_agreement['common_labels']\n"
     ]
    }
   ],
   "source": [
    "# ì¼ì¹˜í•˜ëŠ” ë¼ë²¨ë§Œ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "df_agreement = df[df['common_labels'].map(len) > 0].reset_index(drop=True) # agreement ì»¬ëŸ¼ì´ ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ë§Œ ì„ íƒ\n",
    "\n",
    "# 1ì°¨ ë°ì´í„° csv íŒŒì¼ì—ì„œ 'agreement' ì»¬ëŸ¼ì´ ë¹„ì–´ ìˆì§€ ì•Šì€ í–‰ë§Œ ì„ íƒ\n",
    "df_agreement = df[df['common_labels'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# 'agreement' ì»¬ëŸ¼ì˜ ë¦¬ìŠ¤íŠ¸ë“¤ì„ ìƒˆë¡œìš´ 'labels' ì»¬ëŸ¼ì— í• ë‹¹\n",
    "df_agreement['labels'] = df_agreement['common_labels']\n",
    "df_agreement_reset = df_agreement.reset_index()\n",
    "\n",
    "#  cleaned labelsê°€ ë¹„ì–´ ìˆì§€ ì•Šì€ í–‰ë§Œ í•„í„°ë§ - Define df_agreement_cleaned FIRST\n",
    "df_agreement_cleaned = df_agreement_reset[df_agreement_reset['labels'].map(len) > 0].reset_index(drop = True) # Line 46 (moved up) - df_agreement_cleaned is DEFINED here FIRST\n",
    "\n",
    "# ë¶ˆìš© ë¼ë²¨ ì œê±° (optional): ['nan', '', None] ë¼ë²¨ ì œê±°\n",
    "labels_to_remove = ['nan', '', None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels(labels):\n",
    "    return [label for label in labels if label not in labels_to_remove and pd.notna(label) and label != 'nan']\n",
    "\n",
    "# Assign 'labels_cleaned' column to the ALREADY DEFINED df_agreement_cleaned\n",
    "df_agreement_cleaned['labels_cleaned'] = df_agreement_reset['labels'].apply(remove_labels) # Line 43 (moved down) - Assign to df_agreement_cleaned AFTER it's defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„\n",
    "df_cleaned = df[df[\"common_labels\"].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "texts = df_cleaned[\"ë³¸ë¬¸\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ ë¬¸ì¥ë“¤ì„ Document í˜•íƒœë¡œ ë³€í™˜\n",
    "documents = [Document(page_content=text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    metadata: dict\n",
    "    page_content: str\n",
    "\n",
    "def add_common_labels_to_documents(documents: List[Document], df_cleaned, column_name=\"common_labels\"):\n",
    "    \"\"\"\n",
    "    documentsì˜ metadata ë”•ì…”ë„ˆë¦¬ì— df_cleanedì˜ 'common_labels' ì—´ ê°’ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): Document ê°ì²´ ë¦¬ìŠ¤íŠ¸.\n",
    "        df_cleaned (pd.DataFrame): 'common_labels' ì—´ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°í”„ë ˆì„.\n",
    "        column_name (str): ì¶”ê°€í•  ì—´ ì´ë¦„. ê¸°ë³¸ê°’ì€ 'common_labels'.\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: metadataê°€ ì—…ë°ì´íŠ¸ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸.\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        if i < len(df_cleaned):\n",
    "            # df_cleanedì˜ 'common_labels' ê°’ì„ metadataì— ì¶”ê°€\n",
    "            doc.metadata[column_name] = df_cleaned[column_name].iloc[i]\n",
    "        else:\n",
    "            # df_cleanedì— ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€\n",
    "            doc.metadata[column_name] = []\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•¨ìˆ˜ í˜¸ì¶œ\n",
    "updated_documents = add_common_labels_to_documents(documents, df_cleaned)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "# page_content='ì£½ëŠ” ë‚ ê¹Œì§€ í•˜ëŠ˜ì„ ìš°ëŸ¬ëŸ¬' metadata={'common_labels': ['ë¹„ì¥í•¨']}\n",
    "# page_content='í•œ ì  ë¶€ë„ëŸ¼ì´ ì—†ê¸°ë¥¼,' metadata={'common_labels': ['ë¹„ì¥í•¨', 'ë¶€ë„ëŸ¬ì›€']}\n",
    "# page_content='ììƒˆì— ì´ëŠ” ë°”ëŒì—ë„' metadata={'common_labels': ['ê¸°ëŒ€ê°']}\n",
    "# page_content='ë‚˜ëŠ” ê´´ë¡œì›Œí–ˆë‹¤.' metadata={'common_labels': ['íŒ¨ë°°/ìê¸°í˜ì˜¤', 'ì ˆë§', 'ìŠ¬í””', 'í˜ë“¦/ì§€ì¹¨']}\n",
    "# page_content='ë³„ì„ ë…¸ë˜í•˜ëŠ” ë§ˆìŒìœ¼ë¡œ' metadata={'common_labels': ['ê¸°ì¨', 'ê¸°ëŒ€ê°', 'ì•„ê»´ì£¼ëŠ”']}\n",
    "# page_content='ëª¨ë“  ì£½ì–´ê°€ëŠ” ê²ƒì„ ì‚¬ë‘í•´ì•¼ì§€' metadata={'common_labels': ['ë¹„ì¥í•¨', 'ë¶ˆìŒí•¨/ì—°ë¯¼', 'ì•„ê»´ì£¼ëŠ”']}\n",
    "# page_content='ê±¸ì–´ê°€ì•¼ê² ë‹¤.' metadata={'common_labels': ['ë¹„ì¥í•¨']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class KcELECTRAEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"beomi/KcELECTRA-base\", device: str = \"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self._embed(text).tolist() for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._embed(text).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3ï¸âƒ£ ë²¡í„° ì„ë² ë”© ëª¨ë¸ ë¡œë”© (í•œêµ­ì–´ ì§€ì›í•˜ëŠ” ëª¨ë¸ ê¶Œì¥) KcElectra -> backbone ëª¨ë¸ë¡œ ì‚¬ìš©\n",
    "embedding_model = KcELECTRAEmbeddings()\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sbert-sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4ï¸âƒ£ FAISS VectorDB ìƒì„±\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"../data/poetry_vectorstore\")  # ë²¡í„° DB ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œì»¬ì—ì„œ ë¡œë“œ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì¼ ê²½ìš°)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"../data/poetry_vectorstore\", \n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ ê°ì • ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def generate_prompt_withVector(emotion_scores, context_snippets):\n",
    "    top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "#     return f\"\"\"ë‹¹ì‹ ì€ ê°ì •ì´ ì„¬ì„¸í•œ í•œêµ­ ê·¼í˜„ëŒ€ ì‹œì¸ì…ë‹ˆë‹¤.\n",
    "# '{top_emotion}'ì˜ ê°ì •ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì§§ì€ ì‹œë¥¼ í•œ í¸ ì°½ì‘í•´ ì£¼ì„¸ìš”.\n",
    "# ë‹¤ìŒì€ ê°ì •ì˜ ë¶„ìœ„ê¸°ë¥¼ ë„ì™€ì¤„ ì°¸ê³  ë¬¸ì¥ì…ë‹ˆë‹¤:\n",
    "# {context_snippets} ì´ ë¬¸ì¥ë“¤ì€ 5ëª…ì˜ ì–´ë…¸í…Œì´í„°ê°€ 44ê°œ ê°ì • ë¼ë²¨ ì¤‘ ìµœëŒ€ 10ê°œë¥¼ ì„ íƒí•œ ê²°ê³¼ì—ì„œ 3ì¸ ì´ìƒ ê³µí†µëœ ê°ì •ë§Œ ì¶”ì¶œí•œ ì‹ ë¢°ë„ ë†’ì€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
    "# ì´ ë¬¸ì¥ë“¤ì˜ ì •ì„œì™€ ë‹¨ì–´, ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ ì§€ì–´ ì£¼ì„¸ìš”.\n",
    "# \"\"\"\n",
    "\n",
    "    # emotion_scores = classify_emotion(sample_text)\n",
    "    # emotion_prompt = generate_prompt(emotion_scores)\n",
    "    # poem = chain.run(emotion_prompt=emotion_prompt)\n",
    "    # ê°ì • ë”•ì…”ë„ˆë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    top_emotion = dict_to_text(emotion_scores)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### ì‹œìŠ¤í…œ:\n",
    "    ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹œì ì¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ëª©ë¡ê³¼ ì •ë„ë¥¼ ë…¹ì—¬ë‚¸ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\n",
    "    ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”.\n",
    "    {context_snippets} ì´ ë¬¸ì¥ë“¤ì—ì„œ metadataì—ì„œ ì§€ì •í•œ ê°ì •ë¦¬ìŠ¤íŠ¸ì™€ ê°€ê¹Œìš´ page-contentì˜ ë‹¨ì–´, ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ ì§€ì–´ ì£¼ì„¸ìš”.\n",
    "    ìƒì„±í•œ ì‹œë§Œ ì•Œë ¤ì£¼ì„¸ìš”. ê·¸ ì™¸ì— ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    ### ê°ì • ëª©ë¡: {top_emotion}\n",
    "    ### ì‹œ:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(top_emotion=top_emotion, context_snippets=context_snippets)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8ï¸âƒ£ ì „ì²´ ì²´ì¸\n",
    "def emotion_to_poetry_V(user_input):\n",
    "    scores = classify_emotion(user_input)\n",
    "    # top_emotion = max(scores, key=scores.get)\n",
    "    top_emotion = dict_to_text(scores)\n",
    "    # ê´€ë ¨ ì‹œêµ¬ ê²€ìƒ‰\n",
    "    context_snippets = vectorstore.similarity_search(top_emotion, k=10)  # k=10ë¡œ ì„¤ì •, í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    \n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### ì‹œìŠ¤í…œ:\n",
    "    ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ ê·¼í˜„ëŒ€ ì‹œì¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ëª©ë¡ê³¼ ì •ë„ë¥¼ ë…¹ì—¬ë‚¸ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”. ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n",
    "    {context_snippets} ì´ ë¬¸ì¥ë“¤ì—ì„œ metadataì—ì„œ ì§€ì •í•œ ê°ì •ë¦¬ìŠ¤íŠ¸ì™€ ê°€ê¹Œìš´ page-contentì˜ ë‹¨ì–´, ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ ì§€ì–´ ì£¼ì„¸ìš”.\n",
    "    ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”.\n",
    "    ìƒì„±í•œ ì‹œë§Œ ì•Œë ¤ì£¼ì„¸ìš”. ê·¸ ì™¸ì— ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    ### ê°ì • ëª©ë¡: {top_emotion}\n",
    "    ### ì‹œ:\n",
    "    \"\"\"\n",
    "    \n",
    "    # LangChain Prompt + LLM ì‹¤í–‰\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(top_emotion=top_emotion, context_snippets=context_snippets)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "user_text = \"\"\"\n",
    "ìì„¸íˆ ë³´ì•„ì•¼\n",
    "ì˜ˆì˜ë‹¤\n",
    "ì˜¤ë˜ ë³´ì•„ì•¼\n",
    "ì‚¬ë‘ìŠ¤ëŸ½ë‹¤\n",
    "\n",
    "ë„ˆë„ ê·¸ë ‡ë‹¤.  \n",
    "\"\"\" # ê¸°í˜•ë„ - ë…¸ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample Inference ê²°ê³¼]\n",
      "{'ê°ë™/ê°íƒ„': 0.38199999928474426, 'ê¸°ëŒ€ê°': 0.40400001406669617, 'ê¸°ì¨': 0.4440000057220459, 'ë¶ˆìŒí•¨/ì—°ë¯¼': 0.3330000042915344, 'ì‹ ê¸°í•¨/ê´€ì‹¬': 0.32100000977516174, 'ì•„ê»´ì£¼ëŠ”': 0.6809999942779541, 'íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨)': 0.4399999976158142}\n",
      "ğŸ´ ìƒì„±ëœ ì‹œ:\n",
      " ### ì‹œìŠ¤í…œ:\n",
      "    ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ ê·¼í˜„ëŒ€ ì‹œì¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ê°ì •ëª©ë¡ê³¼ ì •ë„ë¥¼ ë…¹ì—¬ë‚¸ ì§§ì€ í•œêµ­ì–´ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”. ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n",
      "    [Document(id='e4b7d2fd-e5d9-4b08-a009-ee61313a1956', metadata={'common_labels': ['ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 'ê¸°ëŒ€ê°', 'ë¶€ë„ëŸ¬ì›€']}, page_content='í”¼ë ¤ëŠ” ì¥ë¯¸í™”(è–”è–‡èŠ±)ëŠ” ì•„ë‹ˆë¼ë„ ê°ˆì§€ ì•Šì€ ë°±ì˜¥(ç™½ç‰)ì¸ ìˆœê²°í•œ ë‚˜ì˜ ì…ìˆ ì€ ë¯¸ì†Œ(å¾®ç¬‘)ì— ëª©ìš•(æ²æµ´) ê°ëŠ” ê·¸ ì…ìˆ ì— ì±„ ë‹¿ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤'), Document(id='590e4201-c29f-4dc2-ac51-ffa21a1e47e8', metadata={'common_labels': ['ê°ë™/ê°íƒ„', 'ì•„ê»´ì£¼ëŠ”', 'ê¸°ì¨', 'í–‰ë³µ', 'ê¹¨ë‹¬ìŒ']}, page_content='ì²œêµ­(å¤©åœ‹)ì˜ ìŒì•…(éŸ³æ¨‚)ì€ ë‹˜ì˜ ë…¸ë˜ì˜ ë°˜í–¥(åéŸ¿)ì…ë‹ˆë‹¤ ì•„ë¦„ë‹¤ìš´ ë³„ë“¤ì€ ë‹˜ì˜ ëˆˆë¹›ì˜ í™”í˜„(åŒ–ç¾)ì…ë‹ˆë‹¤'), Document(id='7a0a061c-be92-45a2-a321-26e5c7679e8e', metadata={'common_labels': ['ê°ë™/ê°íƒ„', 'ì•„ê»´ì£¼ëŠ”']}, page_content='ê½ƒí–¥ê¸°ì˜ ë¬´ë¥´ë…¹ì€ ì•ˆê°œì— ì·¨(é†‰)í•˜ì—¬ ì²­ì¶˜(é‘æ˜¥)ì˜ ê´‘ì•¼(æ› é‡)ì— ë¹„í‹€ê±¸ìŒì¹˜ëŠ” ë¯¸ì¸(ç¾äºº)ì´ì—¬'), Document(id='c1a646a5-27e6-43bc-bf07-b05099e032b9', metadata={'common_labels': ['ê°ë™/ê°íƒ„', 'ì¡´ê²½', 'ë¹„ì¥í•¨']}, page_content='ì´‰ì„ë£¨(çŸ—çŸ³æ¨“)ë¥¼ ì•ˆê³  ëŒë©° í‘¸ë¥¸ ë¬¼ê²°ì˜ ê·¸ìœ½í•œ í’ˆì— ë…¼ê°œ(è«–ä»‹)ì˜ ì²­ì¶˜(é‘æ˜¥)ì„ ì ì¬ìš°ëŠ” ë‚¨ê°•(å—æ±Ÿ)ì˜ íë¥´ëŠ” ë¬¼ì•„'), Document(id='2f0a561f-4e5e-4cce-b6cc-3159012297f9', metadata={'common_labels': ['ì¡´ê²½', 'ì£„ì±…ê°', 'ë¹„ì¥í•¨']}, page_content='ìš©ì„œ(å®¹æ•)í•˜ì—¬ìš” ë…¼ê°œ(è«–ä»‹)ì—¬ ê·¸ëŒ€ê°€ ìš©ì„œ(å®¹æ•)í•˜ë©´ ë‚˜ì˜ ì£„(ç½ª)ëŠ” ì‹ (ç¥)ì—ê²Œ ì°¸íšŒ(æ‡ºæ‚”)ë¥¼ ì•„ë‹ˆí•œëŒ€ë„ ì‚¬ë¼ì§€ê² ìŠµë‹ˆë‹¤'), Document(id='ff8258fd-d048-47f2-aab5-3af73ca1fbdd', metadata={'common_labels': ['ê¸°ì¨', 'ê°ë™/ê°íƒ„', 'ê¸°ëŒ€ê°', 'ì•„ê»´ì£¼ëŠ”', 'ë¿Œë“¯í•¨']}, page_content='ì²­ì¶˜(é‘æ˜¥)ì˜ ìŒì•…(éŸ³æ¨‚)ì— ë¬´ë„(èˆè¹ˆ)í•˜ëŠ” ë‚˜ì˜ ê°€ìŠ´ì„ íƒœìš°ëŠ” ë¶ˆì€ ê°€ëŠ” ë‹˜ì´ ë‚´ì…¨ìŠµë‹ˆë‹¤'), Document(id='56290555-0d71-41e4-b7cf-8d4504939f37', metadata={'common_labels': ['ì‹ ê¸°í•¨/ê´€ì‹¬', 'ê°ë™/ê°íƒ„']}, page_content='ë¬¼ë‚˜ë¼ì˜ ì˜ë¡±(ç²ç“)í•œ êµ¬ì¤‘ê¶ê¶(ä¹é‡å®®é—•), ê¶ê¶(å®®é—•)ì˜ ì˜¤ìš”í•œ ê³³,'), Document(id='57b98a95-5441-444c-8933-dbb0998be325', metadata={'common_labels': ['ë¶ˆì•ˆ/ê±±ì •', 'ë¶ˆìŒí•¨/ì—°ë¯¼', 'ë¶€ë‹´/ì•ˆ_ë‚´í‚´']}, page_content='íƒœê³ ì˜í˜¸ìˆ˜ë°”íƒ•ì´ë˜ì§€ì ì´ì§œë‹¤. ë§‰ì„ë²„í‹´ê¸°ë‘¥ì´ìŠµí•´ë“¤ì–´ì˜¨ë‹¤. êµ¬ë¦„ì´ê·¼ê²½ì—ì˜¤ì§€ì•Šê³ ì˜¤ë½ì—†ëŠ”ê³µê¸°ì†ì—ì„œê°€ë”í¸ë„ì„ ë“¤ì„ì•“ëŠ”ë‹¤. í™”íì˜ìŠ¤ìº”ë‹¬â”€ë°œì²˜ëŸ¼ìƒê¸´ì†ì´ì—¼ì¹˜ì—†ì´ë…¸íŒŒì˜í†µê³ í•˜ëŠ”ì†ì„ì¡ëŠ”ë‹¤.'), Document(id='4a2000bf-f6d3-4240-af82-8ee3a9cef616', metadata={'common_labels': ['ë¶ˆìŒí•¨/ì—°ë¯¼', 'ë¶ˆì•ˆ/ê±±ì •', 'ê³µí¬/ë¬´ì„œì›€', 'ì•ˆíƒ€ê¹Œì›€/ì‹¤ë§', 'ì„œëŸ¬ì›€']}, page_content='ë‚´ê°€ì¹˜ë˜ê°œ(ç‹—)ëŠ”íŠ¼íŠ¼í•˜ëŒ€ì„œëª¨ì¡°ë¦¬ì‹¤í—˜ë™ë¬¼ë¡œê³µì–‘ë˜ê³ ê·¸ì¤‘ì—ì„œë¹„íƒ€ë¯¼Eë¥¼ì§€ë‹Œê°œ(ç‹—)ëŠ”í•™êµ¬ì˜ë¯¸ê¸‰ê³¼ìƒë¬¼ë‹¤ìš´ì§ˆíˆ¬ë¡œí•´ì„œë°•ì‚¬ì—ê²Œí ì”¬ì–»ì–´ë§ëŠ”ë‹¤í•˜ê³ ì‹¶ì€ë§ì„ê°œì§–ë“¯ë°°ì•ì•„ë†“ë˜ì„¸ì›”ì€ìˆ¨ì—ˆë‹¤. ì˜ê³¼ëŒ€í•™í—ˆì „í•œë§ˆë‹¹ì—ìš°ëšì„œì„œë‚˜ëŠ”í•„ì‚¬ë¡œê¸ˆì œë¥¼ì•“ëŠ”(æ‚£)ë‹¤. ë…¼ë¬¸ì—ì¶œì„í•œì–µìš¸í•œì´‰ë£¨ì—ëŠ”ì²œê³ ì—ì”¨ëª…ì´ì—†ëŠ”ë²•ì´ë‹¤.'), Document(id='f8458115-a934-4d52-ab28-5b698098e914', metadata={'common_labels': ['ë‹¹í™©/ë‚œì²˜', 'ê¹¨ë‹¬ìŒ']}, page_content='ì•„ì•„ ì˜¨ê°– ìœ¤ë¦¬(å€«ç†), ë„ë•(é“å¾·), ë²•ë¥ (æ³•å¾‹)ì€ ì¹¼ê³¼ í™©ê¸ˆ(é»ƒé‡‘)ì„ ì œì‚¬(ç¥­ç¥€) ì§€ë‚´ëŠ” ì—°ê¸°(çƒŸæ°£)ì¸ ì¤„ì„ ì•Œì•˜ìŠµë‹ˆë‹¤')] ì´ ë¬¸ì¥ë“¤ì—ì„œ metadataì—ì„œ ì§€ì •í•œ ê°ì •ë¦¬ìŠ¤íŠ¸ì™€ ê°€ê¹Œìš´ page-contentì˜ ë‹¨ì–´, ì˜›ìŠ¤ëŸ¬ìš´ í•œêµ­ ê³ ìœ ì˜ í‘œí˜„ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ ì§€ì–´ ì£¼ì„¸ìš”.\n",
      "    ì£¼ì–´ì§„ ê°ì • ëª©ë¡ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©í•˜ì—¬ ì°½ì˜ì ìœ¼ë¡œ ê°ì •ì„ í‘œí˜„í•˜ì„¸ìš”.\n",
      "    ìƒì„±í•œ ì‹œë§Œ ì•Œë ¤ì£¼ì„¸ìš”. ê·¸ ì™¸ì— ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
      "    ### ê°ì • ëª©ë¡: ê°ë™/ê°íƒ„: 0.38199999928474426\n",
      "ê¸°ëŒ€ê°: 0.40400001406669617\n",
      "ê¸°ì¨: 0.4440000057220459\n",
      "ë¶ˆìŒí•¨/ì—°ë¯¼: 0.3330000042915344\n",
      "ì‹ ê¸°í•¨/ê´€ì‹¬: 0.32100000977516174\n",
      "ì•„ê»´ì£¼ëŠ”: 0.6809999942779541\n",
      "íë­‡í•¨(ê·€ì—¬ì›€/ì˜ˆì¨): 0.4399999976158142\n",
      "    ### ì‹œ: \n",
      "    ë‹¬ë¹›ì— ë¹„ì¹œ ì •ì›ì€ ë¶€ë“œëŸ½ê²Œ ì–´ë‘¡ê³ , ì •ì›ì˜ ê¹Šì€ ë°”ìœ„ëŠ” í™ê³¼ í•¨ê»˜ ì„¸ìƒì„ ì§€í‚¤ëŠ” ìì‚°ì…ë‹ˆë‹¤. \n",
      "    ì •ì› ì†ì˜ ì†Œë‚˜ê¸°ì™€ ë¬¼ë°©ìš¸ì€ í•œ ë²ˆì— ë‹¤ì†Œ ê±°ì¹œ ë¶„ìœ„ê¸°ë¥¼ ë§Œë“œëŠ” ë° ë„ì›€ì´ ë˜ì§€ë§Œ, ê·¸ë“¤ì˜ ì†Œë¦¬ê°€ ì†Œë‚˜ê¸°ì™€ ë¬¼ë°©ìš¸ì´ ì„œë¡œë¥¼ ë§ì¶”ëŠ” ì†Œë¦¬ê°€ ë” í¬ê²Œ ë“¤ë¦½ë‹ˆë‹¤. \n",
      "    ì •ì› ì†ì˜ ê½ƒì€ ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ë“œëŸ¬ë‚´ëŠ” ëŒ€ì‹ , ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì´ì£ . \n",
      "    ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì€ ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ì ì  ë” ê°€ë ¤ì§€ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. \n",
      "    ì •ì› ì†ì˜ ë‚˜ë¬´ëŠ” ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì´ì£ . \n",
      "    ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì€ ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ì ì  ë” ê°€ë ¤ì§€ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. \n",
      "    ì •ì› ì†ì˜ ë‚˜ë¬´ëŠ” ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì´ì£ . \n",
      "    ì •ì› ì†ì˜ ë‚˜ë¬´ëŠ” ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì´ì£ . \n",
      "    ì •ì› ì†ì˜ ë‚˜ë¬´ëŠ” ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì´ì£ . \n",
      "    ì •ì› ì†ì˜ ë‚˜ë¬´ëŠ” ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì´ì£ . \n",
      "\n",
      "---\n",
      "\n",
      "ì´ ì‹œëŠ” ì •ì› ì†ì˜ ê½ƒê³¼ ë‚˜ë¬´ë“¤ì´ ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì„ ì£¼ì œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì •ì› ì†ì˜ ì†Œë‚˜ê¸°ì™€ ë¬¼ë°©ìš¸ì´ ê·¸ë“¤ì˜ ì†Œë¦¬ë¥¼ ì¡°ìœ¨í•˜ë©°, ì •ì› ì†ì˜ ë‚˜ë¬´ë“¤ì´ ê·¸ë“¤ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ë ¤ëŠ” ê²ƒì„ ìƒì§•í•©ë‹ˆë‹¤. ì‹œëŠ” ì •ì› ì†ì˜ ìì—°ì„ í†µí•´ ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ì ì  ë” ê°€ë ¤ì§€ëŠ” ì•„ë¦„ë‹¤ì›€ì„ í‘œí˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì‹œ:\n",
      "    ì²œêµ­(å¤©åœ‹)ì˜ ìŒì•…(éŸ³æ¨‚)ì€ ë‹˜ì˜ ë…¸ë˜ì˜ ë°˜í–¥(åéŸ¿)ì…ë‹ˆë‹¤ ì•„ë¦„ë‹¤ìš´ ë³„ë“¤ì€ ë‹˜ì˜ ëˆˆë¹›ì˜ í™”í˜„(åŒ–ç¾)ì…ë‹ˆë‹¤.\n",
      "    ì²œêµ­(å¤©åœ‹)ì˜ ìŒì•…(éŸ³æ¥½)ì€ ë‹˜ì˜ ë…¸ë˜ì˜ ë°˜í–¥(åéŸ¿)ì…ë‹ˆë‹¤ ì•„ë¦„ë‹¤ìš´ ë³„ë“¤ì€ ë‹˜ì˜ ëˆˆë¹›ì˜ í™”í˜„(åŒ–ç¾)ì…ë‹ˆë‹¤.\n",
      "    ì²œêµ­(å¤©åœ‹)ì˜ ìŒì•…(éŸ³ä¹)ì€ ë‹˜ì˜ ë…¸ë˜ì˜ ë°˜í–¥(åéŸ¿)ì…ë‹ˆë‹¤ ì•„ë¦„ë‹¤ìš´ ë³„ë“¤ì€ ë‹˜ì˜ ëˆˆë¹›ì˜ í™”í˜„(åŒ–\n"
     ]
    }
   ],
   "source": [
    "generated_poem_V = emotion_to_poetry_V(user_text)\n",
    "\n",
    "print(\"ğŸ´ ìƒì„±ëœ ì‹œ:\\n\", generated_poem_V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
