{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.4.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.11.10)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m462.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, typing-inspection, typing-inspect, tenacity, python-dotenv, marshmallow, jsonpatch, httpx-sse, requests-toolbelt, dataclasses-json, pydantic-settings, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.25 langchain-community-0.3.25 langchain-core-0.3.65 langchain-text-splitters-0.3.8 langsmith-0.3.45 marshmallow-3.26.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "from transformers import ElectraModel, AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path (KOTE Finetuned): ./model/250127_KcElectra_kote.ckpt\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# KOTE 파인튜닝 모델 로드 (체크포인트 파일 경로 수정)\n",
    "###########################\n",
    "best_ckpt_path_kote = './model/250127_KcElectra_kote.ckpt' # Colab 경로에 맞게 수정\n",
    "print(\"Best checkpoint path (KOTE Finetuned):\", best_ckpt_path_kote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOTETagger 클래스는 이전 코드와 동일\n",
    "class KOTETagger(pl.LightningModule): # KOTETagger 클래스 정의 (이전 코드에서 복사)\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.electra = AutoModel.from_pretrained(MODEL_NAME, return_dict=True) # pretrained_electra 제거 및 직접 로드\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, 44) # num_labels=44 (KOTE 라벨 개수)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "        return loss, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, _ = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY) # 가중치 감쇠\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 토크나이저 로드\n",
    "###########################\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\n",
    "    \"감동/감탄\", \"경악\", \"고마움\", \"공포/무서움\", \"귀찮음\", \"기대감\", \"기쁨\", \"깨달음\",\n",
    "    \"놀람\", \"당황/난처\", \"부끄러움\", \"부담/안_내킴\", \"불쌍함/연민\", \"불안/걱정\", \"불평/불만\",\n",
    "    \"비장함\", \"뿌듯함\", \"서러움\", \"슬픔\", \"신기함/관심\", \"아껴주는\", \"안심/신뢰\", \"안타까움/실망\",\n",
    "    \"어이없음\", \"없음\", \"역겨움/징그러움\", \"우쭐댐/무시함\", \"의심/불신\", \"재미없음\", \"절망\",\n",
    "    \"존경\", \"죄책감\", \"즐거움/신남\", \"증오/혐오\", \"지긋지긋\", \"짜증\", \"패배/자기혐오\",\n",
    "    \"편안/쾌적\", \"한심함\", \"행복\", \"화남/분노\", \"환영/호의\", \"흐뭇함(귀여움/예쁨)\", \"힘듦/지침\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kote_finetuned_model = KOTETagger.load_from_checkpoint(best_ckpt_path_kote)\n",
    "pretrained_electra = kote_finetuned_model.electra # 수정: electra backbone만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# LightningModule 정의 (PoetryTagger) (기존 코드 활용 + 가중치 손실 함수, Dropout, Weight Decay, Learning Rate 감소, EarlyStopping patience 증가)\n",
    "###########################\n",
    "INITIAL_LR = 1e-5 # 학습률 감소 (원래 2e-5, 1e-5, 5e-6, 2e-6)\n",
    "DROPOUT_RATE = 0.5 # Dropout 비율 (0.1, 0.3, 0.5) - Dropout 추가\n",
    "WEIGHT_DECAY = 0.02 # Weight Decay 값 (0.001, 0.01, 0.02) - Weight Decay 추가\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "class PoetryTagger(pl.LightningModule):\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None, dropout_rate=DROPOUT_RATE): # dropout_rate hyperparameter\n",
    "        super().__init__()\n",
    "        self.electra = pretrained_electra # 수정: KOTE 파인튜닝 모델의 electra backbone 사용\n",
    "        self.classifier = nn.Sequential( # nn.Sequential 사용하여 dropout layer 추가\n",
    "            nn.Linear(self.electra.config.hidden_size, len(emotion_labels)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ) # Classifier 출력층 크기 자동 조정\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss() # 기본 BCE Loss (가중치 미적용)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        # [CLS] 토큰 기준으로 분류\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(probs, labels)\n",
    "\n",
    "        return loss, probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        loss, _ = self(input_ids, attention_mask, labels) # forward 함수에 weights 제거\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        loss, outputs = self(input_ids, attention_mask, labels) # validation loss는 기존 BCE Loss 사용 (optional)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss} # validation metrics are optional\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR, weight_decay=WEIGHT_DECAY) # 가중치 감쇠\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# # Best checkpoint load & Evaluation (poetry-finetuning) (기존 코드 활용)\n",
    "# ###########################\n",
    "def get_latest_version_dir_poetry(base_dir=\"./lightning_logs/poetry-finetuning-3agreements-only\"): # poetry-weighted-finetuning 로 변경\n",
    "    # version_0, version_1, ... version_50 경로를 모두 찾아 리스트업\n",
    "    version_dirs = glob.glob(os.path.join(base_dir, \"version_*\"))\n",
    "    # 버전 숫자를 기준으로 정렬\n",
    "    version_dirs.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
    "    if not version_dirs:\n",
    "        raise FileNotFoundError(f\"No version_* directories found under '{base_dir}'\")\n",
    "    # 가장 마지막(숫자가 가장 큰) 버전 경로 반환\n",
    "    return version_dirs[-1]\n",
    "\n",
    "def get_latest_checkpoint_poetry(version_dir):\n",
    "    ckpt_dir = os.path.join(version_dir, \"checkpoints\")\n",
    "    ckpt_list = glob.glob(os.path.join(ckpt_dir, \"*.ckpt\"))\n",
    "    ckpt_list.sort()  # 파일명 기준 정렬\n",
    "    if not ckpt_list:\n",
    "        raise FileNotFoundError(f\"No .ckpt found under '{ckpt_dir}'\")\n",
    "    # 가장 마지막 파일(정렬 기준)\n",
    "    return ckpt_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint path (Poetry Weighted Finetuned): ./lightning_logs/poetry-finetuning-3agreements-only/version_0/checkpoints/epoch9-val_loss0.2566.ckpt\n"
     ]
    }
   ],
   "source": [
    "# KPoEM 모델 로드 (local)\n",
    "latest_version_dir_poetry = get_latest_version_dir_poetry(\"./lightning_logs/poetry-finetuning-3agreements-only\") # poetry-weighted-finetuning 로 변경\n",
    "best_ckpt_path_poetry = get_latest_checkpoint_poetry(latest_version_dir_poetry)\n",
    "print(\"Best checkpoint path (Poetry Weighted Finetuned):\", best_ckpt_path_poetry) # poetry-weighted-finetuning 로 변경\n",
    "\n",
    "best_model_poetry = PoetryTagger.load_from_checkpoint(best_ckpt_path_poetry)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_model_poetry.to(device)\n",
    "best_model_poetry.eval()\n",
    "best_model_poetry.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"하루 종일 지친 몸으로만 떠돌다가\n",
    "땅에 떨어져 죽지 못한\n",
    "햇빛들은 줄지어 어디로 가는 걸까\n",
    "\n",
    "웅성웅성 가장 근심스러운 색깔로 서행하며\n",
    "이미 어둠이 깔리는 소각장으로 몰려들어\n",
    "몇 점 폐휴지로 타들어가는 오루 6시의 참혹한 형량\n",
    "단 한 번 후회도 용서하지 않는 무서운 시간\n",
    "바람은 긴 채찍을 휘둘러\n",
    "살아서 빛나는 온갖 상징을 몰아내고 있다.\n",
    "\n",
    "도시는 곧 활자들이 일제히 빠져 달아나\n",
    "속도 없이 페이지를 펄럭이는 텅 빈 한 권 책이 되리라.\n",
    "승부를 알 수 없는 하루와의 싸움에서\n",
    "우리는 패배했을까. 오늘도 물어보는 사소한 물음은\n",
    "그러나 우리의 일생을 텅텅 흔드는 것.\n",
    "\n",
    "오후 6시의 소각장 위로 말없이\n",
    "검은 연기가 우산처럼 펼쳐지고\n",
    "이젠 우리들의 차례였다.\n",
    "두렵지 않은가.\n",
    "밤이면 그림자를 빼앗겨 누구나 아득한 혼자였다.\n",
    "\n",
    "문득 거리를 빠르게 스쳐가는 일상의 공포\n",
    "보여다오. 지금까지 무엇을 했는가 살아 있는 그대여\n",
    "오후 6시 우리들 이마에도 아, 붉은 노을이 떴다.\n",
    "\n",
    "그러면 우리는 어디로 가지?\n",
    "아직도 펄펄 살아 있는 우리는 이제 각자 어디로 가지?\n",
    "\"\"\" # 기형도 - 노을"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_emotion(sample_text):\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sample_text,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 입력 텐서 또한 같은 device로 이동\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        # forward\n",
    "        _, predictions = best_model_poetry(input_ids, attention_mask)  # best_model_poetry 사용\n",
    "\n",
    "    # 추론 결과를 CPU로 가져와 numpy로 변환\n",
    "    predictions = predictions.flatten().cpu().numpy()\n",
    "\n",
    "    # 결과를 딕셔너리로 저장 (숫자값으로 변환)\n",
    "    result_dict = {\n",
    "        label_name: float(round(score, 3))  # np.float32 -> float 변환\n",
    "        for label_name, score in zip(emotion_labels, predictions)\n",
    "        if score > THRESHOLD\n",
    "    }\n",
    "\n",
    "    # 결과 출력\n",
    "    # print(\"\\n[Sample Inference 결과]\")\n",
    "    # print(result_dict)\n",
    "\n",
    "    return result_dict\n",
    "    # 예시 출력\n",
    "    # {'불안/걱정': 0.336, '슬픔': 0.311}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample Inference 결과]\n",
      "{'공포/무서움': 0.42800000309944153, '놀람': 0.30399999022483826, '당황/난처': 0.5070000290870667, '부담/안_내킴': 0.39500001072883606, '불안/걱정': 0.550000011920929, '비장함': 0.3319999873638153, '서러움': 0.35199999809265137, '슬픔': 0.41499999165534973, '신기함/관심': 0.30799999833106995, '안타까움/실망': 0.3070000112056732, '의심/불신': 0.30399999022483826, '힘듦/지침': 0.33799999952316284}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'공포/무서움': 0.42800000309944153,\n",
       " '놀람': 0.30399999022483826,\n",
       " '당황/난처': 0.5070000290870667,\n",
       " '부담/안_내킴': 0.39500001072883606,\n",
       " '불안/걱정': 0.550000011920929,\n",
       " '비장함': 0.3319999873638153,\n",
       " '서러움': 0.35199999809265137,\n",
       " '슬픔': 0.41499999165534973,\n",
       " '신기함/관심': 0.30799999833106995,\n",
       " '안타까움/실망': 0.3070000112056732,\n",
       " '의심/불신': 0.30399999022483826,\n",
       " '힘듦/지침': 0.33799999952316284}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotion(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bllossom 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer_bllossom = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer_bllossom.pad_token = tokenizer_bllossom.eos_token  # Blossom은 pad_token이 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b888052c7084ecf8b59fa8d826bcfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16  # 또는 \"auto\"\n",
    ")\n",
    "\n",
    "# 2. 텍스트 생성 파이프라인\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer_bllossom,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# 3. LangChain용 LLM 래퍼\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 프롬프트 템플릿 설정\n",
    "template = \"\"\"\n",
    "### 시스템:\n",
    "당신은 창의적이고 시적인 한국어 작가입니다. 다음 감정을 표현한 짧은 한국어 시를 써주세요.\n",
    "\n",
    "### 감정: {emotion}\n",
    "### 시:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"emotion\"],\n",
    "    template=template.strip()\n",
    ")\n",
    "# 5. LLMChain 구성\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/1771877612.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(\"슬픔\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 시스템:\n",
      "당신은 창의적이고 시적인 한국어 작가입니다. 다음 감정을 표현한 짧은 한국어 시를 써주세요.\n",
      "\n",
      "### 감정: 슬픔\n",
      "### 시: '그리운 시간'\n",
      "\n",
      "여러 번의 아름다운 시간들이 지났습니다.\n",
      "내 마음속에 남아 있는 것은 슬픔뿐.\n",
      "\n",
      "그곳에서 나는 그들을 기다렸지만,\n",
      "모두가 떠난 후, 나만 남았습니다.\n",
      "\n",
      "나도 그들은 나를 기다렸지만,\n",
      "마침내는 나만을 찾았다고 말했습니다.\n",
      "\n",
      "여기서 나는 혼자서 내 고통을 느끼며,\n",
      "시원한 바람과 함께 슬픔을 담아냈습니다.\n",
      "\n",
      "나는 그들에게 한 마디를 कहन хочу.\n",
      "\"내 마음 속에는 너희가 없음을 느끼는 때까지,\n",
      "내 고통이永遠하지 않을 것이라고.\"\n",
      "\n",
      "그리고 나는 그들의 자리를 차지할 수 있을 것입니다.\n",
      "그들처럼 소중한 시간을 보내면,\n",
      "내 마음속에는 다시 사랑과 행복이 생길 것입니다.\n",
      "\n",
      "물론이 아니지만, 나는 그들처럼,\n",
      "우리 모두가 결국 소중함을 알게 될 것입니다.\n",
      "\n",
      "그럼에도 불구하고, 내 고통은 계속될 것입니다.\n",
      "하지만 그 고통이 내 마음속에 남아 있는 슬픔을 덮는 것일 뿐입니다.\n",
      "\n",
      "나는 그들을 잃었지만,\n",
      "내 마음속에는 그들의 존재가 남아 있습니다.\n",
      "\n",
      "그들은 내 고통을 덤불덤 던져주었고,\n",
      "내 마음속에 슬픔을 남겼습니다.\n",
      "\n",
      "그러나 슬픔은 결국 우리에게는 큰 도움이 됩니다.\n",
      "그들을 잃었음에도 불구하고,\n",
      "내 마음속에는 그들의 존재가 남아 있습니다.\n",
      "\n",
      "나는 그들에게 한 마디를 कहन хочу.\n",
      "\"내 마음속에는 너희가 없음을 느끼는 때까지,\n",
      "내 고통이 永远하지 않을 것이라고.\"\n",
      "\n",
      "그리고 나는 그들의 자리를 차지할 수 있을 것입니다.\n",
      "그들처럼 소중한 시간을 보내면,\n",
      "내 마음속에는 다시 사랑과 행복이 생길 것입니다.\n",
      "\n",
      "물론이 아니지만, 나는 그들처럼,\n",
      "우리 모두가 결국 소중함을 알게 될 것입니다.\n",
      "\n",
      "그럼에도 불구하고, 내 고통은 계속될 것입니다.\n",
      "하지만 그 고통이 내 마음속에 남아 있는 슬픔을 덮는 것일 뿐입니다.\n",
      "\n",
      "나는 그들을 잃었지만,\n",
      "내 마음속에는 그들의 존재가 남아 있습니다.\n",
      "\n",
      "그들은 내 고통을 덤\n"
     ]
    }
   ],
   "source": [
    "# 6. 테스트 실행 - 기본 시 생성확인 \n",
    "result = chain.run(\"슬픔\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 감정 분류 모델 적용하여 Blossom으로 시 생성(Vector DB 미적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💬 2. Blossom Prompt Template\n",
    "poetry_template = \"\"\"\n",
    "### 시스템:\n",
    "당신은 감정 분석 결과에 기반해 시를 창작하는 한국어 시인입니다.\n",
    "다음 감정 분포를 참고하여 시를 한 편 지어주세요.\n",
    "\n",
    "### 감정 분포:\n",
    "{emotion_prompt}\n",
    "\n",
    "### 시:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poetry_prompt = PromptTemplate(input_variables=[\"emotion_prompt\"], template=poetry_template.strip())\n",
    "poetry_chain = LLMChain(llm=llm, prompt=poetry_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 감정 기반 프롬프트 생성\n",
    "def generate_prompt(emotion_scores):\n",
    "    top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "    return f\"\"\"당신은 감정이 섬세한 한국 현대시 작가입니다. \n",
    "'{top_emotion}'의 감정을 중심으로 짧은 시를 한 편 창작해 주세요.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ LangChain 프롬프트 템플릿 구성\n",
    "# prompt = PromptTemplate.from_template(\"{emotion_prompt}\")\n",
    "# 4. 프롬프트 템플릿 설정\n",
    "template = \"\"\"\n",
    "    ### 시스템:\n",
    "    당신은 창의적이고 시적인 작가입니다. 다음 감정목록 정도를 녹여낸 짧은 한국어 시를 써주세요.\n",
    "\n",
    "    ### 감정 목록: {emotion}\n",
    "    ### 시:\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"emotion\"],\n",
    "    template=template.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(d):\n",
    "    return '\\n'.join([f\"{k}: {v}\" for k, v in d.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_poetry_section(template):\n",
    "    # Split the template by \"### 시:\" and extract the part after it\n",
    "    if \"### 시:\" in template:\n",
    "        poetry_section = template.split(\"### 시:\")[1].strip()\n",
    "        # Split by lines and return as a list\n",
    "        poetry_lines = poetry_section.splitlines()\n",
    "        return poetry_lines\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ 전체 흐름 함수\n",
    "def emotion_to_poetry(sample_text): #sample text\n",
    "    emotion_scores = classify_emotion(sample_text)\n",
    "    # emotion_prompt = generate_prompt(emotion_scores)\n",
    "    # poem = chain.run(emotion_prompt=emotion_prompt)\n",
    "    # 감정 딕셔너리를 텍스트로 변환\n",
    "    emotion_text = dict_to_text(emotion_scores)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### 시스템:\n",
    "    당신은 창의적이고 시적인 작가입니다. 다음 감정목록과 정도를 녹여낸 짧은 한국어 시를 써주세요.\n",
    "    주어진 감정 목록을 최대한 그대로 사용하지 말고, 은유와 상징을 사용하여 창의적으로 감정을 표현하세요.\n",
    "    \n",
    "    생성한 시만 알려주세요. 그 외에 설명은 포함하지 마세요.\n",
    "    ### 감정 목록: {emotion}\n",
    "    ### 시:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(emotion=emotion_text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"하루 종일 지친 몸으로만 떠돌다가\n",
    "땅에 떨어져 죽지 못한\n",
    "햇빛들은 줄지어 어디로 가는 걸까\n",
    "\n",
    "웅성웅성 가장 근심스러운 색깔로 서행하며\n",
    "이미 어둠이 깔리는 소각장으로 몰려들어\n",
    "몇 점 폐휴지로 타들어가는 오루 6시의 참혹한 형량\n",
    "단 한 번 후회도 용서하지 않는 무서운 시간\n",
    "바람은 긴 채찍을 휘둘러\n",
    "살아서 빛나는 온갖 상징을 몰아내고 있다.\n",
    "\n",
    "도시는 곧 활자들이 일제히 빠져 달아나\n",
    "속도 없이 페이지를 펄럭이는 텅 빈 한 권 책이 되리라.\n",
    "승부를 알 수 없는 하루와의 싸움에서\n",
    "우리는 패배했을까. 오늘도 물어보는 사소한 물음은\n",
    "그러나 우리의 일생을 텅텅 흔드는 것.\n",
    "\n",
    "오후 6시의 소각장 위로 말없이\n",
    "검은 연기가 우산처럼 펼쳐지고\n",
    "이젠 우리들의 차례였다.\n",
    "두렵지 않은가.\n",
    "밤이면 그림자를 빼앗겨 누구나 아득한 혼자였다.\n",
    "\n",
    "문득 거리를 빠르게 스쳐가는 일상의 공포\n",
    "보여다오. 지금까지 무엇을 했는가 살아 있는 그대여\n",
    "오후 6시 우리들 이마에도 아, 붉은 노을이 떴다.\n",
    "\n",
    "그러면 우리는 어디로 가지?\n",
    "아직도 펄펄 살아 있는 우리는 이제 각자 어디로 가지?\n",
    "\"\"\" # 기형도 - 노을"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample Inference 결과]\n",
      "{'공포/무서움': 0.42800000309944153, '놀람': 0.30399999022483826, '당황/난처': 0.5070000290870667, '부담/안_내킴': 0.39500001072883606, '불안/걱정': 0.550000011920929, '비장함': 0.3319999873638153, '서러움': 0.35199999809265137, '슬픔': 0.41499999165534973, '신기함/관심': 0.30799999833106995, '안타까움/실망': 0.3070000112056732, '의심/불신': 0.30399999022483826, '힘듦/지침': 0.33799999952316284}\n"
     ]
    }
   ],
   "source": [
    "# # 6️⃣ 테스트\n",
    "generated_poem = emotion_to_poetry(sample_text)\n",
    "\n",
    "# print(\"🎴 생성된 시:\\n\")\n",
    "# print(generated_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎴 생성된 시:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어둠 속에서 깊은 대지를 깊이 들어가는 것 같아,',\n",
       " '    무언가의 손길이 느껴지는 곳에 서 있다면,',\n",
       " '    두려움이 깊이 들어오는 것 같아,',\n",
       " '    내 마음 속에 두려움이 깊이 들어오다니,',\n",
       " '    이곳에 있는 모든 것이 무서운 것 같아,',\n",
       " '    무언가가 나를 찾아오고 싶다는 느낌,',\n",
       " '    내 마음속에 불안한 고양이가 뛰어다니다니,',\n",
       " '    어둠 속에서 무언가가 나와서도 할 수 없는 것 같아,',\n",
       " '    무언가가 나를 찾는 것만으로도 충분하다고 생각하다니,',\n",
       " '    이곳에 있는 모든 것이 무서운 것 같아,',\n",
       " '    무언가가 나를 찾는 것만으로도 충분하다고 생각하다니,',\n",
       " '    어둠 속에서 깊은 대지를 깊이 들어가는 것 같아. ',\n",
       " '',\n",
       " '    (시가 길게 이어지면, 주어진 감정 목록을 통해 감정이 깊이 들어오며, 시가 끝나면 다시 시작됩니다.)  ### 1차 분석:',\n",
       " '    1. 시는 창의적으로 감정을 표현하고 있습니다. 주어진 감정 목록을 그대로 반영하는 것은 아닙니다. 대신, 은유와 상징을 사용하여 감정을 표현하였습니다.',\n",
       " '    2. 시는 어둠 속에서의 상황을 주제로 하였으며, 이를 통해 감정이 깊이 들어오게 합니다.',\n",
       " '    3. 시는 계속해서 감정을 반복하며, 시가 끝나면 다시 시작됩니다. 이는 감정이 지속적으로 깊이 들어오는 효과를 줄입니다. ',\n",
       " '',\n",
       " '    ### 2차 분석:',\n",
       " \"    1. 시는 감정의 깊이를 표현하기 위해 '어둠 속에서'라는 상징을 사용하였습니다. 이는 감정의 깊이와 어두운 감정을 연상시키는 데 도움을 줍니다.\",\n",
       " \"    2. '두려움이 깊이 들어오는 것 같아'라는 문장은 감정의 깊이를 강조하였으며, '무언가의 손길이 느껴지는 곳에 서 있다면'이라는 문장은 감정의 원천을 나타냅니다.\",\n",
       " \"    3. '무언가가 나를 찾아오고\"]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"🎴 생성된 시:\\n\")\n",
    "extract_poetry_section(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 DB 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# 근현대시 데이터셋 로드 및 전처리 (기존 코드 활용 + 일치 라벨만 사용)\n",
    "############################ 데이터프레임으로 불러오기 (실제 파일 경로로 수정)\n",
    "df = pd.read_csv(\"../data/총합데이터셋_0601_5인 - 행단위.csv\") # Colab 경로에 맞게 수정\n",
    "\n",
    "# 감정 라벨 데이터를 리스트로 변환하는 함수\n",
    "def labels_to_list(labels_str):\n",
    "    if pd.isna(labels_str):\n",
    "        return []\n",
    "    return [label.strip() for label in labels_str.split(',')]\n",
    "\n",
    "# 라벨 데이터를 리스트로 변환\n",
    "df['annotator01_label_list'] = df['annotator01'].apply(labels_to_list)\n",
    "df['annotator02_label_list'] = df['annotator02'].apply(labels_to_list)\n",
    "df['annotator03_label_list'] = df['annotator03'].apply(labels_to_list)\n",
    "df['annotator04_label_list'] = df['annotator04'].apply(labels_to_list)\n",
    "df['annotator05_label_list'] = df['annotator05'].apply(labels_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_agreed_by_at_least_k(row, k=3):\n",
    "    \"\"\"\n",
    "    각 행(row)에 대해, 최소 k명 이상이 동의한 감정만 추출\n",
    "\n",
    "    Parameters:\n",
    "    - row: annotator label list들이 있는 DataFrame row\n",
    "    - k: 동의한 annotator 최소 수 (기본 2명)\n",
    "\n",
    "    Returns:\n",
    "    - 감정 리스트 중 k명 이상이 공통으로 선택한 감정 리스트\n",
    "    \"\"\"\n",
    "    all_labels = (\n",
    "        row['annotator01_label_list'] +\n",
    "        row['annotator02_label_list'] +\n",
    "        row['annotator03_label_list'] +\n",
    "        row['annotator04_label_list'] +\n",
    "        row['annotator05_label_list']\n",
    "    )\n",
    "    counter = pd.Series(all_labels).value_counts() # 감정별 개수 세기\n",
    "    return [label for label, count in counter.items() if count >= k] # k명 이상이 동의한 감정 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_agreed_by_at_least_k(row, k=3):\n",
    "    \"\"\"\n",
    "    각 행(row)에 대해, 최소 k명 이상이 동의한 감정만 추출\n",
    "\n",
    "    Parameters:\n",
    "    - row: annotator label list들이 있는 DataFrame row\n",
    "    - k: 동의한 annotator 최소 수 (기본 2명)\n",
    "\n",
    "    Returns:\n",
    "    - 감정 리스트 중 k명 이상이 공통으로 선택한 감정 리스트\n",
    "    \"\"\"\n",
    "    all_labels = (\n",
    "        row['annotator01_label_list'] +\n",
    "        row['annotator02_label_list'] +\n",
    "        row['annotator03_label_list'] +\n",
    "        row['annotator04_label_list'] +\n",
    "        row['annotator05_label_list']\n",
    "    )\n",
    "    counter = pd.Series(all_labels).value_counts() # 감정별 개수 세기\n",
    "    return [label for label, count in counter.items() if count >= k] # k명 이상이 동의한 감정 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[비장함]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[비장함, 부끄러움]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[기대감]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[패배/자기혐오, 절망, 슬픔, 힘듦/지침]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[기쁨, 기대감, 아껴주는]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[비장함, 불쌍함/연민, 아껴주는]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[비장함]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[기대감, 감동/감탄]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[슬픔]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              common_labels\n",
       "0                     [비장함]\n",
       "1               [비장함, 부끄러움]\n",
       "2                     [기대감]\n",
       "3  [패배/자기혐오, 절망, 슬픔, 힘듦/지침]\n",
       "4           [기쁨, 기대감, 아껴주는]\n",
       "5       [비장함, 불쌍함/연민, 아껴주는]\n",
       "6                        []\n",
       "7                     [비장함]\n",
       "8              [기대감, 감동/감탄]\n",
       "9                      [슬픔]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2명 이상 동의한 감정 리스트로 새 컬럼 생성\n",
    "df['common_labels'] = df.apply(lambda row: get_labels_agreed_by_at_least_k(row, k=3), axis=1)\n",
    "\n",
    "df[['common_labels']].head(10) # 3명 이상 동의한 감정 리스트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/3686308332.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_agreement['labels'] = df_agreement['common_labels']\n"
     ]
    }
   ],
   "source": [
    "# 일치하는 라벨만 있는 데이터만 필터링\n",
    "df_agreement = df[df['common_labels'].map(len) > 0].reset_index(drop=True) # agreement 컬럼이 비어있지 않은 행만 선택\n",
    "\n",
    "# 1차 데이터 csv 파일에서 'agreement' 컬럼이 비어 있지 않은 행만 선택\n",
    "df_agreement = df[df['common_labels'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# 'agreement' 컬럼의 리스트들을 새로운 'labels' 컬럼에 할당\n",
    "df_agreement['labels'] = df_agreement['common_labels']\n",
    "df_agreement_reset = df_agreement.reset_index()\n",
    "\n",
    "#  cleaned labels가 비어 있지 않은 행만 필터링 - Define df_agreement_cleaned FIRST\n",
    "df_agreement_cleaned = df_agreement_reset[df_agreement_reset['labels'].map(len) > 0].reset_index(drop = True) # Line 46 (moved up) - df_agreement_cleaned is DEFINED here FIRST\n",
    "\n",
    "# 불용 라벨 제거 (optional): ['nan', '', None] 라벨 제거\n",
    "labels_to_remove = ['nan', '', None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels(labels):\n",
    "    return [label for label in labels if label not in labels_to_remove and pd.notna(label) and label != 'nan']\n",
    "\n",
    "# Assign 'labels_cleaned' column to the ALREADY DEFINED df_agreement_cleaned\n",
    "df_agreement_cleaned['labels_cleaned'] = df_agreement_reset['labels'].apply(remove_labels) # Line 43 (moved down) - Assign to df_agreement_cleaned AFTER it's defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링된 데이터프레임\n",
    "df_cleaned = df[df[\"common_labels\"].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "texts = df_cleaned[\"본문\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 문장들을 Document 형태로 변환\n",
    "documents = [Document(page_content=text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    metadata: dict\n",
    "    page_content: str\n",
    "\n",
    "def add_common_labels_to_documents(documents: List[Document], df_cleaned, column_name=\"common_labels\"):\n",
    "    \"\"\"\n",
    "    documents의 metadata 딕셔너리에 df_cleaned의 'common_labels' 열 값을 추가하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): Document 객체 리스트.\n",
    "        df_cleaned (pd.DataFrame): 'common_labels' 열을 포함하는 데이터프레임.\n",
    "        column_name (str): 추가할 열 이름. 기본값은 'common_labels'.\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: metadata가 업데이트된 Document 객체 리스트.\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        if i < len(df_cleaned):\n",
    "            # df_cleaned의 'common_labels' 값을 metadata에 추가\n",
    "            doc.metadata[column_name] = df_cleaned[column_name].iloc[i]\n",
    "        else:\n",
    "            # df_cleaned에 없는 경우 빈 리스트 추가\n",
    "            doc.metadata[column_name] = []\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 호출\n",
    "updated_documents = add_common_labels_to_documents(documents, df_cleaned)\n",
    "\n",
    "# 결과 출력\n",
    "# page_content='죽는 날까지 하늘을 우러러' metadata={'common_labels': ['비장함']}\n",
    "# page_content='한 점 부끄럼이 없기를,' metadata={'common_labels': ['비장함', '부끄러움']}\n",
    "# page_content='잎새에 이는 바람에도' metadata={'common_labels': ['기대감']}\n",
    "# page_content='나는 괴로워했다.' metadata={'common_labels': ['패배/자기혐오', '절망', '슬픔', '힘듦/지침']}\n",
    "# page_content='별을 노래하는 마음으로' metadata={'common_labels': ['기쁨', '기대감', '아껴주는']}\n",
    "# page_content='모든 죽어가는 것을 사랑해야지' metadata={'common_labels': ['비장함', '불쌍함/연민', '아껴주는']}\n",
    "# page_content='걸어가야겠다.' metadata={'common_labels': ['비장함']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class KcELECTRAEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"beomi/KcELECTRA-base\", device: str = \"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls_embedding.squeeze().cpu().numpy()\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self._embed(text).tolist() for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._embed(text).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3️⃣ 벡터 임베딩 모델 로딩 (한국어 지원하는 모델 권장) KcElectra -> backbone 모델로 사용\n",
    "embedding_model = KcELECTRAEmbeddings()\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sbert-sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ FAISS VectorDB 생성\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"../data/poetry_vectorstore\")  # 벡터 DB 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에서 로드 (신뢰할 수 있는 파일일 경우)\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"../data/poetry_vectorstore\", \n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ 감정 기반 프롬프트 생성\n",
    "def generate_prompt_withVector(emotion_scores, context_snippets):\n",
    "    top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "#     return f\"\"\"당신은 감정이 섬세한 한국 근현대 시인입니다.\n",
    "# '{top_emotion}'의 감정을 중심으로 한국어로 짧은 시를 한 편 창작해 주세요.\n",
    "# 다음은 감정의 분위기를 도와줄 참고 문장입니다:\n",
    "# {context_snippets} 이 문장들은 5명의 어노테이터가 44개 감정 라벨 중 최대 10개를 선택한 결과에서 3인 이상 공통된 감정만 추출한 신뢰도 높은 데이터셋입니다.\n",
    "# 이 문장들의 정서와 단어, 옛스러운 한국 고유의 표현을 사용하여 시를 지어 주세요.\n",
    "# \"\"\"\n",
    "\n",
    "    # emotion_scores = classify_emotion(sample_text)\n",
    "    # emotion_prompt = generate_prompt(emotion_scores)\n",
    "    # poem = chain.run(emotion_prompt=emotion_prompt)\n",
    "    # 감정 딕셔너리를 텍스트로 변환\n",
    "    top_emotion = dict_to_text(emotion_scores)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### 시스템:\n",
    "    당신은 창의적이고 시적인 작가입니다. 다음 감정목록과 정도를 녹여낸 짧은 한국어 시를 써주세요.\n",
    "    주어진 감정 목록을 최대한 그대로 사용하지 말고, 은유와 상징을 사용하여 창의적으로 감정을 표현하세요.\n",
    "    {context_snippets} 이 문장들에서 metadata에서 지정한 감정리스트와 가까운 page-content의 단어, 옛스러운 한국 고유의 표현을 주로 사용하여 시를 지어 주세요.\n",
    "    생성한 시만 알려주세요. 그 외에 설명은 포함하지 마세요.\n",
    "    ### 감정 목록: {top_emotion}\n",
    "    ### 시:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(top_emotion=top_emotion, context_snippets=context_snippets)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ 전체 체인\n",
    "def emotion_to_poetry_V(user_input):\n",
    "    scores = classify_emotion(user_input)\n",
    "    # top_emotion = max(scores, key=scores.get)\n",
    "    top_emotion = dict_to_text(scores)\n",
    "    # 관련 시구 검색\n",
    "    context_snippets = vectorstore.similarity_search(top_emotion, k=10)  # k=10로 설정, 필요에 따라 조정 가능\n",
    "\n",
    "    # 프롬프트 생성\n",
    "    \n",
    "    \n",
    "    template = \"\"\"\n",
    "    ### 시스템:\n",
    "    당신은 창의적이고 감성적인 근현대 시인입니다. 다음 감정목록과 정도를 녹여낸 짧은 한국어 시를 써주세요. 영어나 다른 언어는 사용하지 말고, 한국어로만 작성해 주세요.\n",
    "    {context_snippets} 이 문장들에서 metadata에서 지정한 감정리스트와 가까운 page-content의 단어, 옛스러운 한국 고유의 표현을 주로 사용하여 시를 지어 주세요.\n",
    "    주어진 감정 목록을 최대한 그대로 사용하지 말고, 은유와 상징을 사용하여 창의적으로 감정을 표현하세요.\n",
    "    생성한 시만 알려주세요. 그 외에 설명은 포함하지 마세요.\n",
    "    ### 감정 목록: {top_emotion}\n",
    "    ### 시:\n",
    "    \"\"\"\n",
    "    \n",
    "    # LangChain Prompt + LLM 실행\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"emotion\"],\n",
    "        template=template.strip()\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(top_emotion=top_emotion, context_snippets=context_snippets)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9️⃣ 테스트 실행\n",
    "user_text = \"\"\"\n",
    "자세히 보아야\n",
    "예쁘다\n",
    "오래 보아야\n",
    "사랑스럽다\n",
    "\n",
    "너도 그렇다.  \n",
    "\"\"\" # 기형도 - 노을\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sample Inference 결과]\n",
      "{'감동/감탄': 0.38199999928474426, '기대감': 0.40400001406669617, '기쁨': 0.4440000057220459, '불쌍함/연민': 0.3330000042915344, '신기함/관심': 0.32100000977516174, '아껴주는': 0.6809999942779541, '흐뭇함(귀여움/예쁨)': 0.4399999976158142}\n",
      "🎴 생성된 시:\n",
      " ### 시스템:\n",
      "    당신은 창의적이고 감성적인 근현대 시인입니다. 다음 감정목록과 정도를 녹여낸 짧은 한국어 시를 써주세요. 영어나 다른 언어는 사용하지 말고, 한국어로만 작성해 주세요.\n",
      "    [Document(id='e4b7d2fd-e5d9-4b08-a009-ee61313a1956', metadata={'common_labels': ['안타까움/실망', '기대감', '부끄러움']}, page_content='피려는 장미화(薔薇花)는 아니라도 갈지 않은 백옥(白玉)인 순결한 나의 입술은 미소(微笑)에 목욕(沐浴) 감는 그 입술에 채 닿지 못하였습니다'), Document(id='590e4201-c29f-4dc2-ac51-ffa21a1e47e8', metadata={'common_labels': ['감동/감탄', '아껴주는', '기쁨', '행복', '깨달음']}, page_content='천국(天國)의 음악(音樂)은 님의 노래의 반향(反響)입니다 아름다운 별들은 님의 눈빛의 화현(化現)입니다'), Document(id='7a0a061c-be92-45a2-a321-26e5c7679e8e', metadata={'common_labels': ['감동/감탄', '아껴주는']}, page_content='꽃향기의 무르녹은 안개에 취(醉)하여 청춘(靑春)의 광야(曠野)에 비틀걸음치는 미인(美人)이여'), Document(id='c1a646a5-27e6-43bc-bf07-b05099e032b9', metadata={'common_labels': ['감동/감탄', '존경', '비장함']}, page_content='촉석루(矗石樓)를 안고 돌며 푸른 물결의 그윽한 품에 논개(論介)의 청춘(靑春)을 잠재우는 남강(南江)의 흐르는 물아'), Document(id='2f0a561f-4e5e-4cce-b6cc-3159012297f9', metadata={'common_labels': ['존경', '죄책감', '비장함']}, page_content='용서(容恕)하여요 논개(論介)여 그대가 용서(容恕)하면 나의 죄(罪)는 신(神)에게 참회(懺悔)를 아니한대도 사라지겠습니다'), Document(id='ff8258fd-d048-47f2-aab5-3af73ca1fbdd', metadata={'common_labels': ['기쁨', '감동/감탄', '기대감', '아껴주는', '뿌듯함']}, page_content='청춘(靑春)의 음악(音樂)에 무도(舞蹈)하는 나의 가슴을 태우는 불은 가는 님이 내셨습니다'), Document(id='56290555-0d71-41e4-b7cf-8d4504939f37', metadata={'common_labels': ['신기함/관심', '감동/감탄']}, page_content='물나라의 영롱(玲瓏)한 구중궁궐(九重宮闕), 궁궐(宮闕)의 오요한 곳,'), Document(id='57b98a95-5441-444c-8933-dbb0998be325', metadata={'common_labels': ['불안/걱정', '불쌍함/연민', '부담/안_내킴']}, page_content='태고의호수바탕이던지적이짜다. 막을버틴기둥이습해들어온다. 구름이근경에오지않고오락없는공기속에서가끔편도선들을앓는다. 화폐의스캔달─발처럼생긴손이염치없이노파의통고하는손을잡는다.'), Document(id='4a2000bf-f6d3-4240-af82-8ee3a9cef616', metadata={'common_labels': ['불쌍함/연민', '불안/걱정', '공포/무서움', '안타까움/실망', '서러움']}, page_content='내가치던개(狗)는튼튼하대서모조리실험동물로공양되고그중에서비타민E를지닌개(狗)는학구의미급과생물다운질투로해서박사에게흠씬얻어맞는다하고싶은말을개짖듯배앝아놓던세월은숨었다. 의과대학허전한마당에우뚝서서나는필사로금제를앓는(患)다. 논문에출석한억울한촉루에는천고에씨명이없는법이다.'), Document(id='f8458115-a934-4d52-ab28-5b698098e914', metadata={'common_labels': ['당황/난처', '깨달음']}, page_content='아아 온갖 윤리(倫理), 도덕(道德), 법률(法律)은 칼과 황금(黃金)을 제사(祭祀) 지내는 연기(烟氣)인 줄을 알았습니다')] 이 문장들에서 metadata에서 지정한 감정리스트와 가까운 page-content의 단어, 옛스러운 한국 고유의 표현을 주로 사용하여 시를 지어 주세요.\n",
      "    주어진 감정 목록을 최대한 그대로 사용하지 말고, 은유와 상징을 사용하여 창의적으로 감정을 표현하세요.\n",
      "    생성한 시만 알려주세요. 그 외에 설명은 포함하지 마세요.\n",
      "    ### 감정 목록: 감동/감탄: 0.38199999928474426\n",
      "기대감: 0.40400001406669617\n",
      "기쁨: 0.4440000057220459\n",
      "불쌍함/연민: 0.3330000042915344\n",
      "신기함/관심: 0.32100000977516174\n",
      "아껴주는: 0.6809999942779541\n",
      "흐뭇함(귀여움/예쁨): 0.4399999976158142\n",
      "    ### 시: \n",
      "    달빛에 비친 정원은 부드럽게 어둡고, 정원의 깊은 바위는 흙과 함께 세상을 지키는 자산입니다. \n",
      "    정원 속의 소나기와 물방울은 한 번에 다소 거친 분위기를 만드는 데 도움이 되지만, 그들의 소리가 소나기와 물방울이 서로를 맞추는 소리가 더 크게 들립니다. \n",
      "    정원 속의 꽃은 그들의 아름다움을 드러내는 대신, 그들의 아름다움을 가려는 것이죠. \n",
      "    그들의 아름다움은 시간이 지나면서 점점 더 가려지는 것 같습니다. \n",
      "    정원 속의 나무는 그들의 아름다움을 가려는 것이죠. \n",
      "    그들의 아름다움은 시간이 지나면서 점점 더 가려지는 것 같습니다. \n",
      "    정원 속의 나무는 그들의 아름다움을 가려는 것이죠. \n",
      "    정원 속의 나무는 그들의 아름다움을 가려는 것이죠. \n",
      "    정원 속의 나무는 그들의 아름다움을 가려는 것이죠. \n",
      "    정원 속의 나무는 그들의 아름다움을 가려는 것이죠. \n",
      "\n",
      "---\n",
      "\n",
      "이 시는 정원 속의 꽃과 나무들이 그들의 아름다움을 가려는 것을 주제로 하고 있습니다. 정원 속의 소나기와 물방울이 그들의 소리를 조율하며, 정원 속의 나무들이 그들의 아름다움을 가려는 것을 상징합니다. 시는 정원 속의 자연을 통해 시간이 지나면서 점점 더 가려지는 아름다움을 표현하고 있습니다.\n",
      "\n",
      "### 시:\n",
      "    천국(天國)의 음악(音樂)은 님의 노래의 반향(反響)입니다 아름다운 별들은 님의 눈빛의 화현(化現)입니다.\n",
      "    천국(天國)의 음악(音楽)은 님의 노래의 반향(反響)입니다 아름다운 별들은 님의 눈빛의 화현(化現)입니다.\n",
      "    천국(天國)의 음악(音乐)은 님의 노래의 반향(反響)입니다 아름다운 별들은 님의 눈빛의 화현(化\n"
     ]
    }
   ],
   "source": [
    "generated_poem_V = emotion_to_poetry_V(user_text)\n",
    "\n",
    "print(\"🎴 생성된 시:\\n\", generated_poem_V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
